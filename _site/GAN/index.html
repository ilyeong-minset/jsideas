<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Learning GAN - jsideas</title>


  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="jsideas" property="og:site_name">
  
    <meta content="Learning GAN" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="a novice's journey into data science
" property="og:description">
  
  
    <meta content="http://localhost:4000/GAN/" property="og:url">
  
  
    <meta content="2017-07-01T09:00:00+09:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/20170701.png" property="og:image">
  
  
    
  
  
    
    <meta content="python" property="article:tag">
    
    <meta content="deep learning" property="article:tag">
    
    <meta content="image" property="article:tag">
    
    <meta content="gan" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@junsik_whang">
  
    <meta name="twitter:title" content="Learning GAN">
  
  
    <meta name="twitter:url" content="http://localhost:4000/GAN/">
  
  
    <meta name="twitter:description" content="a novice's journey into data science
">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/20170701.png">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<!-- <link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon"> -->
	<!-- <link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png"> -->
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/author.jpg" alt="Junsik Hwang"></a>
      </div>
      <div class="author-name">Junsik Hwang</div>
      <p>I do data analytics and modelling for a living and for fun</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/junsik_whang" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/junkwhinger" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/jswhang" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:junsik.whang@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2018 &copy; Junsik Hwang</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/20170701.png alt="Learning GAN">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Learning GAN</h1>
        <div class="page-date"><span>2017, Jul 01&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <h2 id="learning-gan">Learning GAN</h2>

<h3 id="1-what-is-gan">1. What is GAN?</h3>

<p>딥러닝은 어디에 쓸까. 알파고나 자율주행차 등이 딥러닝 사례로 심심찮게 등장하지만, 게임 로그 분석을 하는 입장에서는 실제로는 성능 좋은 분류기로 사용하는 편이다. 그래서인지 Udacity Deep Learning Foundation 후반부에 나오는 GAN은 조금 막연하게 다가온다. 입력 데이터를 학습해 그와 유사하지만 기존에 존재하지 않는 완전히 새로운 데이터를 만들어내는 것은 확실히 재밌는 분야긴 하지만, 개인적으로나 업무적으로 아직 그 활용성이 와닿지는 않는 듯 하다. 어쨌든 배워놓으면 실이 될 것은 없고, 또 뒤에 나올 Semi-Supervised Learning에 GAN을 활용해먹을 수 있으니, 배운 내용을 간단히 남겨본다.</p>

<p>GAN은 Ian GoodFellow가 선보인 모델로, Generative한 모델이다. 예전에도 RNN을 사용해 텍스트를 만들어냈었는데, 이 방식은 단어나 문자를 한번에 하나씩 찍어내는 방식이었다. 이전에 만든 사례에서는 글자 시퀀스를 넣었을 때 그 다음에 오는 글자를 예측하는 식으로 모델이 작동했다. 이와 달리 GAN은 한방에 결과를 만들어내는 방식이다. 이미지를 예로 들자면, 픽셀을 연속적으로 찍어내는 것이 아니라 이미지를 한번에 찍어낸다.</p>

<h3 id="2-gan의-작동방식">2. GAN의 작동방식</h3>

<p>GAN의 구조는 화폐위조범과 경찰의 사례로 쉽게 이해할 수 있다. 화폐위조범은 최대한 실제 화폐에 가깝게 위폐를 찍어내고, 경찰은 그런 화폐를 실제 화폐와 구분하는 역할을 수행한다 (범인을 실제로 잡지는 않는다). 범인은 처음에는 엉성한 화폐를 만들어 경찰에 쉽게 걸리지만, 걸린 결과를 피드백으로 받아 학습하여 점점 퀄리티를 개선해나간다. 개선된 위폐로 인해 경찰의 진폐 분류 확률이 0.5로 떨어지면, 즉 진짜를 분류할 수 없게되면 GAN의 목표를 달성하게 된다.</p>

<p>이같은 흥미로운 작동방식으로 인해 GAN은 2개의 뉴럴네트워크로 구성된다. 앞단인 Generator(화폐위조범)가 랜덤한 벡터인 Z로부터 가짜 데이터를 만든다. 만들어진 가짜 데이터는 진짜 데이터와 섞이고, Discriminator(경찰)가 이를 진짜와 가짜로 분류한다. Discriminator가 분류한 결과는 다시 네트워크를 거슬러 Generator를 학습시킨다.</p>

<p>즉, 데이터를 생성하는 과정은 입력값이었던 랜덤벡터 Z를 X 분포에 맵핑하는 과정이라 생각해볼 수 있다. Z를 계속 X에 집어넣다보면 어떤 분포가 만들어진다. Discriminator에서는 이 분포를 실제 값과 비교한다. 그리고 실제 값의 밀도가 가짜 값의 밀도보다 큰 경우, Discriminator는 가짜를 진짜에서 성공적으로 분류하게 된다. Generator는 이 비교값을 전달받아 학습하는데, 이로서 Generator가 만들어내는 X분포가 실제 분포쪽으로 점진적으로 이동하고, 학습이 완료되면 그 분포의 차이가 거의 없게 되는 것이다.</p>

<h3 id="3-gan-사용-팁">3. GAN 사용 팁</h3>

<p>다른 딥러닝 모델과 마찬가지로 GAN도 튜닝을 잘 해줘야 잘 돌아간다. 아래는 Udacity 코스에서 소개한 몇가지 팁.</p>

<ul>
  <li>Genenator와 Discriminator에 각각 은닉층 한개 이상 둬야-
    <ul>
      <li>그래야 두개 모델 모두 Universal Appromixator Property를 갖게 된다고 한다.</li>
      <li>Generator는 실제 값의 분포를 모사해야 하고, Discriminator는 그 분포의 차이를 추론해야 하니, 이를 적절히 수행할 수 있는 Universal Approximation Function이 필요한 것으로 이해된다.</li>
    </ul>
  </li>
  <li>히든레이어에는 보통 Leaky ReLU를 사용한다.
    <ul>
      <li>Relu는 음수를 0으로, 나머지는 x를 반환하는 함수다.</li>
      <li>Leaky ReLU는 f(x) = max(x, ax)로 표현되는데, a를 0.01 정도로 작은 숫자를 넣는다.</li>
      <li>그렇게 되면 x가 양수라면 x가 반환되고, x가 음수라면, a를 곱해 매우 작아진 음수가 반환된다.</li>
      <li>결과적으로 해당 unit이 active 상태가 아니라도 약간의 gradient가 전달되게 된다.</li>
    </ul>
  </li>
  <li>Generator 아웃풋에는 tanh, Discriminator 아웃풋에는 sigmoid를 쓴다.</li>
  <li>Discriminator loss를 계산할 때는 sigmoid를 통과하기 전인 logits을 쓸 것.</li>
  <li>GAN에서는 레이블 스무딩을 사용한다.
    <ul>
      <li>가짜 데이터는 0, 실제 데이터는 1로 레이블을 단다.</li>
      <li>GAN에서는 더 안정적인 모델을 만들기 위해 1에 0.9 등을 곱해 조금 작게 만들어준다.</li>
    </ul>
  </li>
  <li>d_loss와 g_loss
    <ul>
      <li>d_loss는 Discriminator loss, g_loss는 Generator loss다.</li>
      <li>GAN의 목표는 g_loss와 d_loss 모두 작게 만드는 것.</li>
      <li>직관적으로는 생성기의 성능이 좋을수록 분류기의 성능이 안좋아질 것이므로 서로 음의 관계라 할 수 있다.</li>
      <li>그러나 최적화를 시킬때 -(d_loss)를 g_loss로 쓰면 구현이 잘 안된다.</li>
      <li>궁극적으로 우리는 GAN이 d_loss와 g_loss를 모두 낮게 가져가길 원하므로, G를 학습시킬때는 레이블을 반대로 달아주고 최소 loss를 찾도록 학습시킨다.</li>
    </ul>
  </li>
  <li>이미지를 생성할 때는 Convolutional Network를 반대로 구현하면 된다.
    <ul>
      <li>CNN 분류기는 3채널의 이미지를 점점 작게, 그리고 피쳐맵을 점점 많이 만든다.</li>
      <li>이와 반대로 이미지를 생성할때는 랜덤벡터Z로 구성된 매트릭스를 점점 넓고 피쳐맵을 좁게 만든다.</li>
      <li>즉 반대로 가는 개념. 이는 DCGAN을 실습할 때 더 살펴보기로 한다.</li>
    </ul>
  </li>
  <li>Batch Normalization
    <ul>
      <li>꼭 해야 GAN이 제대로 동작한다.</li>
      <li>DCGAN의 저자는 Generator의 마지막 레이어와 Discriminator의 인풋 레이어를 제외한 모든 레이어에 사용하라 권한다.</li>
      <li>생성된 데이터와 실제 데이터에 따로따로 적용하라 하는데, 이론적으로는 조금 수상하다 생각할 수 있다.</li>
      <li>결국 실제와 생성 데이터에 서로 다른 분포를 가진 Discriminator 함수를 적용하는 꼴이기 때문이다.</li>
      <li>하지만 실제로는 그렇게 해야 더 성능이 좋다고 한다..</li>
    </ul>
  </li>
</ul>

<h3 id="4-실습---mnist-데이터셋">4. 실습 - MNIST 데이터셋</h3>

<p>국민 머신러닝 데이터셋인 MNIST를 넣어 GAN을 돌려보자.
Udacity 코스 자료를 가져다 코멘트를 추가했다.</p>

<h5 id="데이터셋-준비">데이터셋 준비</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">pickle</span> <span class="k">as</span> <span class="n">pkl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">'MNIST_data'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</code></pre></div></div>

<h5 id="모델-입력값">모델 입력값</h5>

<p>그래프에 집어넣을 입력값을 만든다. 네트워크가 2개이므로 넣을 입력값도 2개다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_inputs</span><span class="p">(</span><span class="n">real_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">):</span>
    <span class="s">"""
    G와 D에 넣을 입력값인 inputs_real과 inputs_z를 리턴하는 함수
    
    Arguments
    ---------
    real_dim: 실제 인풋의 형태
    z_dim: 랜덤벡터 Z의 형태
    
    Returns
    -------
    inputs_real: D에 넣을 입력값
    inputs_z: G에 넣을 입력값
    """</span>
    <span class="n">inputs_real</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">real_dim</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_real'</span><span class="p">)</span>
    <span class="n">inputs_z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'input_z'</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">inputs_real</span><span class="p">,</span> <span class="n">inputs_z</span>
</code></pre></div></div>

<h5 id="generator">Generator</h5>

<p>이미지를 생성할 Generator를 구성한다. 입력층 - Leaky Relu를 사용한 은닉층 - tanh를 사용하는 출력층으로 구성한다.</p>

<p>여기서 <code class="highlighter-rouge">tf.variable_scope</code>가 사용되는데, 이를 <code class="highlighter-rouge">generator</code>라고 지정하면, 그 범위내에 생성되는 모든 변수는 그 이름이 앞에 붙게 된다. <code class="highlighter-rouge">discriminator</code>로 마찬가지로 지정하면 나중에 따로따로 변수를 학습시키기에 매우 편리하다.</p>

<p><code class="highlighter-rouge">tf.name_scope</code>도 비슷한 기능을 수행하나, 나중에 다른 입력값만 넣어 네트워크를 재사용해야 하기 때문에 <code class="highlighter-rouge">tf.variable_scope</code>를 사용해야 한다. 우리는 Generator를 학습시키면서도, 학습 도중과 끝난 후에 샘플을 채집해야 한다. 또 Discriminator는 가짜와 진짜 이미지간에 변수를 공유해야 한다. 이때 <code class="highlighter-rouge">tf.variable_scope</code>가 매우 편리하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generator</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="s">''' 
    Generator 네트워크를 만든다.
    
    Arguments
    ---------
    z : Generator에 넣을 입력값(텐서)
    out_dim : 출력될 결과물의 형태
    n_units : 은닉층 유닛 갯수
    reuse : 재사용 여부
    alpha : Leaky ReLU에 넣을 leak 파라미터

    Returns
    -------
    out: 생성 결과
    '''</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'generator'</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
        <span class="c"># Hidden layer</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">n_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        
        <span class="c"># Leaky ReLU</span>
        <span class="c">## tf에 별도의 함수가 없어 이렇게 구현해야 한다.</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h1</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">h1</span><span class="p">)</span>
        
        <span class="c"># Logits and tanh output</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">h1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<h5 id="discriminator">Discriminator</h5>

<p>Discriminator는 Generator와 거의 비슷하다. 출력층에 tanh 대신 sigmoid를 쓴다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">discriminator</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="s">''' 
    Discriminator 네트워크를 만든다.
    
    Arguments
    ---------
    x : Discriminator에 넣을 입력값
    n_units : 은닉층 유닛 갯수
    reuse : 재사용 여부
    alpha : Leaky ReLU에 넣을 leak 파라미터

    Returns
    -------
    out: 분류 결과
    logits: sigmoid 직전 logits
    '''</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'discriminator'</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
        <span class="c"># Hidden layer</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        
        <span class="c"># Leaky ReLU</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h1</span><span class="p">)</span>
        
        <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="c">## sigmoid를 쓴다.</span>
        
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">logits</span>
</code></pre></div></div>

<h5 id="hyperparameters">Hyperparameters</h5>

<p>GAN 학습에 필요한 파라미터를 정의한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># D에 넣을 입력 데이터의 크기 (MNIST는 28*28인데 784개로 구성된 벡터로 변환해 넣으므로)</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="mi">784</span>
<span class="c"># 랜덤벡터Z의 크기</span>
<span class="n">z_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c"># G와 D의 은닉층 유닛 개수</span>
<span class="n">g_hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">d_hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="c"># Leaky ReLU에 넣을 Leak 파라미터</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c"># 레이블 스무딩 파라미터</span>
<span class="n">smooth</span> <span class="o">=</span> <span class="mf">0.1</span>
</code></pre></div></div>

<h5 id="build-network">Build network</h5>

<p>그래프를 만들고, 입력값, Generator 모델, Discriminator 모델을 정의한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
<span class="c"># 입력값을 정의.</span>
<span class="n">input_real</span><span class="p">,</span> <span class="n">input_z</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">z_size</span><span class="p">)</span>

<span class="c"># Generator 모델을 구현한다.</span>
<span class="n">g_model</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">input_z</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="n">g_hidden_size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

<span class="c"># Discriminator 모델을 구현한다.</span>
<span class="c">## 여기서는 진짜와 가짜 2개를 만든다.</span>
<span class="c">## 진짜와 가짜는 모두 같은 네트워크 가중치를 사용해야 하므로, reuse=True를 인자로 넘긴다.</span>
<span class="n">d_model_real</span><span class="p">,</span> <span class="n">d_logits_real</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">input_real</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="n">d_hidden_size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
<span class="n">d_model_fake</span><span class="p">,</span> <span class="n">d_logits_fake</span> <span class="o">=</span> <span class="n">discriminator</span><span class="p">(</span><span class="n">g_model</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_units</span><span class="o">=</span><span class="n">d_hidden_size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="discriminator-and-generator-losses">Discriminator and Generator Losses</h5>

<p>loss를 계산해야 하는데 이부분이 조금 어렵다. Discriminator의 loss는 진짜 데이터의 loss와 가짜의 loss의 합계다. 각각의 sigmoid_cross_entropy를 구할 때 labels에 진짜는 1을 가짜는 0을 지정하면 된다.</p>

<p>Generator loss는 가짜 이미지의 로짓인 d_logits_fake를 사용하지만, 앞서 언급했듯, label을 1로 지정해서 구한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Calculate losses</span>
<span class="c">## d_logits_real과 1의 차이가 진짜 데이터의 loss가 된다.</span>
<span class="n">d_loss_real</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">d_logits_real</span><span class="p">,</span> 
                                                          <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">d_logits_real</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">smooth</span><span class="p">)))</span>

<span class="c">## d_logits_fake와 0의 차이가 가짜 데이터의 loss가 된다.                                                          </span>
<span class="n">d_loss_fake</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">d_logits_fake</span><span class="p">,</span> 
                                                          <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">d_logits_real</span><span class="p">)))</span>
                                                          
<span class="c">## d_loss는 진짜와 가짜 loss의 합이다.</span>
<span class="n">d_loss</span> <span class="o">=</span> <span class="n">d_loss_real</span> <span class="o">+</span> <span class="n">d_loss_fake</span>

<span class="c">## g_loss는 가짜 loss와 1(가짜지만, Discriminator와 반대이므로 1)의 차이로 구한다.</span>
<span class="n">g_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">sigmoid_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">d_logits_fake</span><span class="p">,</span>
                                                     <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">d_logits_fake</span><span class="p">)))</span>
</code></pre></div></div>

<h5 id="optimizers">Optimizers</h5>

<p>Generater와 Discriminator에 사용하는 변수는 따로따로 업데이트해줘야 한다.</p>

<p><code class="highlighter-rouge">tf.trainable_variables()</code>는 그래프에 선언한 모든 변수를 담은 리스트를 반환한다.</p>

<p>Generator Optimizer에서는 당연히 generator 변수만 필요하다. 앞에서 <code class="highlighter-rouge">tf.variable_scope</code>로 이름을 할당해두었으므로, 리스트에서 generator로 시작하는 변수만 따로 추리면 된다. discriminator도 같은 방식으로 추린다.</p>

<p>그리고나서, optimizer의 minimize 함수에 var_list로 변수 리스트를 전달한다. 이로서 optimizer는 전달된 변수만을 학습한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Optimizers</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.002</span>

<span class="c"># 전체 변수를 가져온 다음, 이름으로 G와 D에 들어갈 변수를 추린다.</span>
<span class="n">t_vars</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">()</span>
<span class="n">g_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">var</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">t_vars</span> <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'generator'</span><span class="p">)]</span>
<span class="n">d_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">var</span> <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">t_vars</span> <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'discriminator'</span><span class="p">)]</span>

<span class="n">d_train_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">d_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">d_vars</span><span class="p">)</span>
<span class="n">g_train_opt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">g_loss</span><span class="p">,</span> <span class="n">var_list</span><span class="o">=</span><span class="n">g_vars</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="training">Training</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c"># Generator 변수만 저장한다.</span>
<span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="n">g_vars</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c">## 모든 변수를 이니셜라이즈한다.</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
    <span class="c">## epoch을 돌면서...</span>
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c">## 미니배치를 돌면서</span>
        <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">num_examples</span><span class="o">//</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            
            <span class="c"># 실제 이미지를 가져와서 형태와 스케일을 바꾼다 (-&gt; D)</span>
            <span class="n">batch_images</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
            <span class="n">batch_images</span> <span class="o">=</span> <span class="n">batch_images</span><span class="o">*</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
            
            <span class="c"># 랜덤벡터 Z를 만든다 (-&gt; G)</span>
            <span class="n">batch_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">z_size</span><span class="p">))</span>
            
            <span class="c"># optimizer를 돌린다.</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">d_train_opt</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_real</span><span class="p">:</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">input_z</span><span class="p">:</span> <span class="n">batch_z</span><span class="p">})</span>
            <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">g_train_opt</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_z</span><span class="p">:</span> <span class="n">batch_z</span><span class="p">})</span>
        
        <span class="c"># 각 epoch의 끝에 loss를 출력한다.</span>
        <span class="n">train_loss_d</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">d_loss</span><span class="p">,</span> <span class="p">{</span><span class="n">input_z</span><span class="p">:</span> <span class="n">batch_z</span><span class="p">,</span> <span class="n">input_real</span><span class="p">:</span> <span class="n">batch_images</span><span class="p">})</span>
        <span class="n">train_loss_g</span> <span class="o">=</span> <span class="n">g_loss</span><span class="o">.</span><span class="nb">eval</span><span class="p">({</span><span class="n">input_z</span><span class="p">:</span> <span class="n">batch_z</span><span class="p">})</span>
            
        <span class="k">print</span><span class="p">(</span><span class="s">"Epoch {}/{}..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span>
              <span class="s">"Discriminator Loss: {:.4f}..."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_loss_d</span><span class="p">),</span>
              <span class="s">"Generator Loss: {:.4f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_loss_g</span><span class="p">))</span>    
        <span class="c"># 시각화를 위해 저장해둔다.</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">train_loss_d</span><span class="p">,</span> <span class="n">train_loss_g</span><span class="p">))</span>
        
        <span class="c"># 결과 시각화를 위해 샘플을 추출한다.</span>
        <span class="n">sample_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">z_size</span><span class="p">))</span>
        <span class="n">gen_samples</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                       <span class="n">generator</span><span class="p">(</span><span class="n">input_z</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                       <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_z</span><span class="p">:</span> <span class="n">sample_z</span><span class="p">})</span>
        <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gen_samples</span><span class="p">)</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">'./checkpoints_20170701/generator.ckpt'</span><span class="p">)</span>

<span class="c"># 생성한 샘플을 저장한다.</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'train_samples_20170701.pkl'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pkl</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/100... Discriminator Loss: 0.3542... Generator Loss: 4.2156
Epoch 2/100... Discriminator Loss: 0.4807... Generator Loss: 3.5386
Epoch 3/100... Discriminator Loss: 0.4184... Generator Loss: 3.0571
Epoch 4/100... Discriminator Loss: 0.6623... Generator Loss: 2.4477
Epoch 5/100... Discriminator Loss: 0.4933... Generator Loss: 4.4115
Epoch 6/100... Discriminator Loss: 4.0790... Generator Loss: 4.2294
Epoch 7/100... Discriminator Loss: 1.5767... Generator Loss: 2.8047
Epoch 8/100... Discriminator Loss: 4.1314... Generator Loss: 2.4155
Epoch 9/100... Discriminator Loss: 1.5074... Generator Loss: 1.2608
Epoch 10/100... Discriminator Loss: 1.2602... Generator Loss: 1.1861
...
Epoch 90/100... Discriminator Loss: 0.9186... Generator Loss: 2.1974
Epoch 91/100... Discriminator Loss: 1.0408... Generator Loss: 1.8543
Epoch 92/100... Discriminator Loss: 1.1210... Generator Loss: 1.9848
Epoch 93/100... Discriminator Loss: 1.0030... Generator Loss: 1.8941
Epoch 94/100... Discriminator Loss: 1.1500... Generator Loss: 1.7497
Epoch 95/100... Discriminator Loss: 1.0165... Generator Loss: 2.0192
Epoch 96/100... Discriminator Loss: 1.3226... Generator Loss: 1.9656
Epoch 97/100... Discriminator Loss: 1.0169... Generator Loss: 1.3822
Epoch 98/100... Discriminator Loss: 1.0336... Generator Loss: 1.9438
Epoch 99/100... Discriminator Loss: 0.9182... Generator Loss: 1.8705
Epoch 100/100... Discriminator Loss: 0.9737... Generator Loss: 2.0093
</code></pre></div></div>

<h5 id="training-loss">Training loss</h5>

<p>Generator와 Discriminator의 loss가 어떻게 달라지는지 보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Discriminator'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Generator'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Training Losses"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x123dde320&gt;
</code></pre></div></div>

<p><img src="/assets/materials/20170701/Learning_GAN_25_1.png" alt="png" /></p>

<h5 id="생성한-결과물-보기">생성한 결과물 보기</h5>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">view_samples</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">samples</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="n">epoch</span><span class="p">]):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Greys_r'</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Load samples from generator taken while training</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'train_samples_20170701.pkl'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">pkl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span> <span class="o">=</span> <span class="n">view_samples</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/materials/20170701/Learning_GAN_29_0.png" alt="png" /></p>

<p>샘플로 16개를 만들어보았다. 1번째와 3번째 줄의 결과는 약간 애매하지만 전반적으로 사람이 알아볼 수 있을 정도다. 총 100개 epoch을 돌렸는데 10개마다 어떻게 생성 퀄리티가 개선되는지 살펴보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="n">rows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sample</span><span class="p">,</span> <span class="n">ax_row</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">samples</span><span class="p">[::</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="o">/</span><span class="n">rows</span><span class="p">)],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sample</span><span class="p">[::</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span><span class="o">/</span><span class="n">cols</span><span class="p">)],</span> <span class="n">ax_row</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Greys_r'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/materials/20170701/Learning_GAN_31_0.png" alt="png" /></p>

<p>처음에는 전혀 알아볼 수 없는 노이즈가 출력되지만, epoch이 거듭될수록 숫자의 형태가 명확해지는 것을 볼 수 있다.</p>

<h5 id="sampling-from-the-generator">Sampling from the generator</h5>

<p>마지막으로 학습이 끝난 generator를 활용하여 완전히 새로운 결과를 만들어보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">var_list</span><span class="o">=</span><span class="n">g_vars</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="s">'checkpoints'</span><span class="p">))</span>
    <span class="n">sample_z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">z_size</span><span class="p">))</span>
    <span class="n">gen_samples</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                   <span class="n">generator</span><span class="p">(</span><span class="n">input_z</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                   <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">input_z</span><span class="p">:</span> <span class="n">sample_z</span><span class="p">})</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">view_samples</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="n">gen_samples</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/assets/materials/20170701/Learning_GAN_34_0.png" alt="png" /></p>

<p>마지막 그림처럼 완전히 알아보기 어려운 것도 존재는 하지만, 전체적으로 꽤 괜찮은 결과가 나왔다. Convolutional network를 쓰지 않고도, epoch을 100번만 돌리고도 이정도 퀄리티가 나온 것은 꽤 고무적인 성과다. 다음에는 DCGAN을 살펴보자.</p>


      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Learning GAN&url=http://localhost:4000/GAN/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/GAN/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/GAN/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#python" class="tag">&#35; python</a>
          
            <a href="/tags#deep learning" class="tag">&#35; deep learning</a>
          
            <a href="/tags#image" class="tag">&#35; image</a>
          
            <a href="/tags#gan" class="tag">&#35; gan</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
    <div id="disqus_thread" class="article-comments"></div>
    <script>
      (function() {
          var d = document, s = d.createElement('script');
          s.src = '//jsideas.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-36651119-2', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>
