<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Self Attention: Name Classifier</title>
  <meta name="description" content="a novice's journey into data science
" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <link rel="canonical" href="http://localhost:4000/python/2018/09/01/name_classifier-copy.html">

  <link rel="shortcut icon" href="/assets/images/favicon.ico">
<!--  <link rel="stylesheet" href=""> -->
  <link rel="stylesheet" href="http://brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
</head>

  <body itemscope itemtype="http://schema.org/Article">
    <!-- header start -->

<a href="http://localhost:4000" class="logo-readium"><span class="logo" style="background-image: url(/assets/images/df_logo.jpg)"></span></a>

<!-- header end -->

    <main class="content" role="main">
      <article class="post">
        
        <div class="article-image">
          <div class="post-image-image" style="background-image: url(/assets/selfattention/variants_of_junsik.png)">
            Article Image
          </div>
          <div class="post-meta">
            <h1 class="post-title">Self Attention: Name Classifier</h1>
            <div class="cf post-meta-text">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person">Junsik Whang</h4>
              on
              <time datetime="2018-09-01 09:00">01 Sep 2018</time>
              <!-- , tagged on <span class="post-tag-">, <a href="/tag/"></a></span> -->
            </div>
            <div style="text-align:center">
              <a href="#topofpage" class="topofpage"><i class="fa fa-angle-down"></i></a>
            </div>
          </div>
        </div>
        
        <section class="post-content">
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
          <a name="topofpage"></a>
          <h1 id="self-attention-name-classifier">Self Attention: Name Classifier</h1>

<h2 id="intro">Intro</h2>

<p>On hearing a person’s name, we can often correctly tell if it’s he or she. Male names tend to have strong pronunciations like Mark, Robert, and Lucas. On the other hand, female names are likely to sound smoother like Lucy, Stella, and Valerie. Would a neural network be able to replicate our classification process? And what part of names would it pay attention to mainly?</p>

<h2 id="dataset">Dataset</h2>

<p>For this personal research project, I crawled commonly used baby names that are freely available on the internet. Some of them had metadata like origin and popularities.</p>

<h3 id="example">example</h3>

<table>
  <thead>
    <tr>
      <th>babyname</th>
      <th>sex</th>
      <th>origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Aakesh</td>
      <td>boy</td>
      <td>Indian</td>
    </tr>
    <tr>
      <td>Aaren</td>
      <td>boy</td>
      <td>Hebrew</td>
    </tr>
    <tr>
      <td>Abalina</td>
      <td>girl</td>
      <td>Hebrew</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="exploration">Exploration</h3>

<p>Let’s dive into the dataset and find out some useful patterns.</p>

<h4 id="most-frequently-used-first-letter">Most frequently used first letter</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>A - 3,792건 (10.2%)</td>
      <td>A - 2,118건 (9.7%)</td>
      <td>A - 1,674건 (10.8%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>C - 2,887건 (7.7%)</td>
      <td>C - 1,880건 (8.6%)</td>
      <td>B - 1,051건 (6.8%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>S - 2,862건 (7.7%)</td>
      <td>S - 1,825건 (8.4%)</td>
      <td>M - 1,051건 (6.8%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M - 2,615건 (7.0%)</td>
      <td>M - 1,564건 (7.2%)</td>
      <td>S - 1,037건 (6.7%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>K - 2,262건 (6.1%)</td>
      <td>K - 1,531건 (7.0%)</td>
      <td>C - 1,007건 (6.5%)</td>
    </tr>
  </tbody>
</table>

<p>A is the most commonly chosen first letter in both sexes. B stands out among the boys’ first letters.</p>

<h4 id="most-frequently-used-first-two-letters">Most frequently used first two letters</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Ma - 1,454건 (3.9%)</td>
      <td>Ma - 907건 (4.2%)</td>
      <td>Ma - 547건 (3.5%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Ka - 914건 (2.4%)</td>
      <td>Ka - 726건 (3.3%)</td>
      <td>Ha - 368건 (2.4%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Sh - 864건 (2.3%)</td>
      <td>Sh - 715건 (3.3%)</td>
      <td>Al - 336건 (2.2%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Ca - 856건 (2.3%)</td>
      <td>Ca - 624건 (2.9%)</td>
      <td>Da - 305건 (2.0%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Al - 842건 (2.3%)</td>
      <td>Ch - 600건 (2.8%)</td>
      <td>De - 300건 (1.9%)</td>
    </tr>
  </tbody>
</table>

<p>M pops up in the top first two letters. The difference between sexes is a little bit more prominent than the first letter chart.</p>

<h4 id="most-dominant-first-two-letters-by-sex">Most dominant first two letters by sex</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>first_two_letters</th>
      <th>Girl_ratio</th>
      <th>Boy_ratio</th>
      <th>Absolute Difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Sh</td>
      <td>3.3%</td>
      <td>1.0%</td>
      <td>2.3%</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Ka</td>
      <td>3.3%</td>
      <td>1.2%</td>
      <td>2.1%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Ch</td>
      <td>2.8%</td>
      <td>1.3%</td>
      <td>1.5%</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Ha</td>
      <td>0.9%</td>
      <td>2.4%</td>
      <td>1.5%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Ca</td>
      <td>2.9%</td>
      <td>1.5%</td>
      <td>1.4%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Ba</td>
      <td>0.5%</td>
      <td>1.7%</td>
      <td>1.2%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>Ga</td>
      <td>0.4%</td>
      <td>1.4%</td>
      <td>1.0%</td>
    </tr>
    <tr>
      <td>8</td>
      <td>Jo</td>
      <td>1.8%</td>
      <td>1.0%</td>
      <td>0.8%</td>
    </tr>
    <tr>
      <td>9</td>
      <td>La</td>
      <td>2.0%</td>
      <td>1.3%</td>
      <td>0.7%</td>
    </tr>
    <tr>
      <td>10</td>
      <td>Co</td>
      <td>0.9%</td>
      <td>1.6%</td>
      <td>0.7%</td>
    </tr>
  </tbody>
</table>

<p>Sh, Ka, Ch, La, Jo are used more frequently in female names than male names. Male names prefer Ha, Ba, Ga, Co as their opening sequences. But the ratio gaps don’t seem to be widened significantly.</p>

<h4 id="most-frequently-used-last-letter">Most frequently used last letter</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>a - 10,693건 (28.6%)</td>
      <td>a - 10,198건 (46.8%)</td>
      <td>n - 3,123건 (20.1%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>e - 6,471건 (17.3%)</td>
      <td>e - 4,989건 (22.9%)</td>
      <td>o - 1,519건 (9.8%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>n - 4,813건 (12.9%)</td>
      <td>n - 1,690건 (7.8%)</td>
      <td>e - 1,482건 (9.5%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>y - 1,992건 (5.3%)</td>
      <td>y - 1,036건 (4.8%)</td>
      <td>s - 1,448건 (9.3%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>s - 1,825건 (4.9%)</td>
      <td>i - 927건 (4.3%)</td>
      <td>r - 1,123건 (7.2%)</td>
    </tr>
  </tbody>
</table>

<p>The last letter might hold some clues.
Nearly half of the female names end with A. A fifth of them have E ending. These endings are not as dominant in male names as in female names.</p>

<h4 id="most-dominant-last-letter-by-sex">Most dominant last letter by sex</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>last_letter</th>
      <th>Girl_ratio</th>
      <th>Boy_ratio</th>
      <th>Absolute Difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>a</td>
      <td>46.8%</td>
      <td>3.2%</td>
      <td>43.6%</td>
    </tr>
    <tr>
      <td>2</td>
      <td>e</td>
      <td>22.9%</td>
      <td>9.5%</td>
      <td>13.4%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>n</td>
      <td>7.8%</td>
      <td>20.1%</td>
      <td>12.3%</td>
    </tr>
    <tr>
      <td>4</td>
      <td>o</td>
      <td>0.8%</td>
      <td>9.8%</td>
      <td>9.0%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>s</td>
      <td>1.7%</td>
      <td>9.3%</td>
      <td>7.6%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>r</td>
      <td>0.9%</td>
      <td>7.2%</td>
      <td>6.3%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>d</td>
      <td>0.7%</td>
      <td>5.9%</td>
      <td>5.2%</td>
    </tr>
    <tr>
      <td>8</td>
      <td>l</td>
      <td>2.6%</td>
      <td>6.6%</td>
      <td>4.0%</td>
    </tr>
    <tr>
      <td>9</td>
      <td>k</td>
      <td>0.1%</td>
      <td>2.7%</td>
      <td>2.6%</td>
    </tr>
    <tr>
      <td>10</td>
      <td>t</td>
      <td>1.5%</td>
      <td>4.0%</td>
      <td>2.5%</td>
    </tr>
  </tbody>
</table>

<h4 id="most-frequently-used-last-two-letters">Most frequently used last two letters</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>na - 2,419건 (6.5%)</td>
      <td>na - 2,375건 (10.9%)</td>
      <td>on - 890건 (5.7%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>ne - 1,713건 (4.6%)</td>
      <td>ne - 1,476건 (6.8%)</td>
      <td>an - 763건 (4.9%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ia - 1,510건 (4.0%)</td>
      <td>ia - 1,472건 (6.8%)</td>
      <td>er - 558건 (3.6%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>ie - 1,022건 (2.7%)</td>
      <td>la - 909건 (4.2%)</td>
      <td>in - 482건 (3.1%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>on - 1,000건 (2.7%)</td>
      <td>ta - 837건 (3.8%)</td>
      <td>us - 476건 (3.1%)</td>
    </tr>
  </tbody>
</table>

<h4 id="most-dominant-last-two-letters-by-sex">Most dominant last two letters by sex</h4>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>last_letter</th>
      <th>Girl_ratio</th>
      <th>Boy_ratio</th>
      <th>Absolute Difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>na</td>
      <td>10.9%</td>
      <td>0.3%</td>
      <td>10.6%</td>
    </tr>
    <tr>
      <td>2</td>
      <td>ia</td>
      <td>6.8%</td>
      <td>0.2%</td>
      <td>6.6%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ne</td>
      <td>6.8%</td>
      <td>1.5%</td>
      <td>5.3%</td>
    </tr>
    <tr>
      <td>4</td>
      <td>on</td>
      <td>0.5%</td>
      <td>5.7%</td>
      <td>5.2%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>la</td>
      <td>4.2%</td>
      <td>0.2%</td>
      <td>4.0%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>an</td>
      <td>0.9%</td>
      <td>4.9%</td>
      <td>4.0%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>ta</td>
      <td>3.8%</td>
      <td>0.2%</td>
      <td>3.6%</td>
    </tr>
    <tr>
      <td>8</td>
      <td>ra</td>
      <td>3.6%</td>
      <td>0.2%</td>
      <td>3.4%</td>
    </tr>
    <tr>
      <td>9</td>
      <td>er</td>
      <td>0.4%</td>
      <td>3.6%</td>
      <td>3.2%</td>
    </tr>
    <tr>
      <td>10</td>
      <td>da</td>
      <td>3.3%</td>
      <td>0.1%</td>
      <td>3.2%</td>
    </tr>
  </tbody>
</table>

<p>The simple aggregations above revealed that the last one or two letters might influence how we tell male names from female names. Would a neural classifier perform better than simple rules? Would it also pay attention to how the names end?</p>

<h2 id="model">Model</h2>

<p>Text classification models usually extract patterns from text via RNN or CNN layers before processing them through a single or multiple fully connected layers to obtain probability values for each class.</p>

<p><img src="/assets/selfattention/basic_lstm_classifier.png" alt="basic_lstm_classifier" /></p>

<p>BiLSTM model above does perform well, but it does not tell us the evidence that it takes into account. I could use occlusion methods like Local Interpretable Model-Agnostic Explanations(LIME), but these methods run the model multiple times with masked inputs to figure out the part that contributes the most. There must be a more straight-forward and convenient way to do this.</p>

<h3 id="a-structure-self-attentive-sentence-embedding">A Structure Self-Attentive Sentence Embedding</h3>

<p>A Structured Self-Attentive Sentence Embedding (2017.03) introduces Self-Attention to instill visibility into the text model.</p>

<p><img src="/assets/selfattention/self_attention.png" alt="self_attention" /></p>

<p>The self-attention model uses the same BiLSTM feature extractor as an ordinary text classifier. It passes the feature through two fully connected layers (W_s1, W_s2) to achieve Attention matrix whose shape is n_token x hops. <code class="highlighter-rouge">da</code> for W_s1 and <code class="highlighter-rouge">hops</code> for W_s2 are hyperparameters. Compared to the conventional attention mechanisms that output an attention vector, the attention matrix has the following benefits:</p>
<ul>
  <li>Multiple attention vectors represent multiple features that a sentence has.</li>
  <li>The model does not need extra inputs to obtain attention.</li>
  <li>Attention alleviates the burden of LSTM as it accesses all the time steps of the LSTM layer.</li>
  <li>Attention matrix is incredibly easy to visualize as a heat map whose size is hops x n_token.</li>
</ul>

<p>A softmax operation is included in the attention part to normalize each vector so that the visualized heat map would be understood intuitively.</p>

<p>To obtain the classification result, the model multiplies the LSTM output with the attention matrix before flattening and fully connected layers. Some pruning methods are suggested by the paper to decrease the trainable parameters, but I skipped this part as my model itself was not big enough to be pruned.</p>

<h3 id="loss-function">Loss Function</h3>

<p>In addition to the cross-entropy loss for classification error, the paper introduces another loss function called <code class="highlighter-rouge">Penalization Term</code>. The attention matrix is composed of multiple vectors that focus on specific parts of the input sentence. It will be of a huge waste if several attention vectors end up looking at the same area. Penalization term prevents this from happening.</p>

<table>
  <tbody>
    <tr>
      <td>$ P =</td>
      <td> </td>
      <td>(AA^T - I )</td>
      <td> </td>
      <td>_F^2 $</td>
    </tr>
  </tbody>
</table>

<p>The attention matrix $A$ is multiplied with its own transposed matrix before subtracting an identity matrix. The penalization term is the Frobenius norm of the operation.</p>

<h2 id="experiment">Experiment</h2>

<h3 id="training-setting">Training Setting</h3>

<p>The authors carried out some interesting experiments such as classifying emotions of reviews and predicting ages of Twitterians by their tweets. The fundamental component of the inputs used in these experiments is words.</p>

<p>However, in my experiment, the basic unit is alphabets. As the diversity of alphabet letters is far less complicated than of English word I thought the hyperparameters used in the paper would be much of an overkill to classify names.</p>

<p>So I reduced some of the hyperparameters to 1/10 or 1/5 of the ones suggested by the paper.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "num_epochs": 10,
    "batch_size": 16,
    "save_summary_steps": 100,
    "learning_rate": 0.001,
    "weight_decay": 0.0001,
    "embedding_dim": 100,
    "hidden_dim": 300,
    "nb_layers": 1,
    "nb_hops": 5,
    "da": 30,
    "fc_ch": 300,
    "nb_classes": 2,
    "device": "cpu",
    "train_size": 33609,
    "val_size": 3735,
    "vocab_size": 35,
    "coef": 0.5,
    "isPenalize": 1,
    "dropout": 0.0,
    "model": "selfattention"
}
</code></pre></div></div>

<h3 id="model-performance">Model Performance</h3>

<p>분류 성능 비교를 위해 논문의 베이스라인 모델 중 하나인 BiLSTM + MaxPooling 모델을 만들었다. Self Attention 모델과 같은 하이퍼파라미터를 사용했다 (사용하지 않는 레이어 제외).</p>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>Validation Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BiLSTM + MaxPooling (epoch 8)</td>
      <td>0.884</td>
    </tr>
    <tr>
      <td>Self Attention (epoch 10)</td>
      <td>0.892</td>
    </tr>
  </tbody>
</table>

<p>In line with the experimental results of the paper, the self-attention model outperformed the baseline model regarding validation accuracy. The training accuracy was about the same, indicating that the model learned generalized representations.</p>

<p>By class…</p>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>Precision(girl)</th>
      <th>Precision(boy)</th>
      <th>Recall(girl)</th>
      <th>Recall(boy)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BiLSTM + MaxPooling</td>
      <td>0.901</td>
      <td>0.860</td>
      <td>0.900</td>
      <td>0.862</td>
    </tr>
    <tr>
      <td>Self Attention</td>
      <td>0.903</td>
      <td>0.876</td>
      <td>0.913</td>
      <td>0.862</td>
    </tr>
  </tbody>
</table>

<p>Neither of them produced biased predictions.</p>

<h2 id="visualizing-self-attention">Visualizing Self Attention</h2>

<h3 id="attention-heatmap">Attention Heatmap</h3>

<p>And here comes the hidden purpose of this blog post. Which part of the names did the model pay attention to before making the final decision? As explained above, each row vector of the attention matrix sums up to 1. All I have to do is to pass the numpy array to matplotlib.</p>

<p><img src="/assets/selfattention/attention_heatmap.png" alt="attention_heatmap" /></p>

<p>The heat map above shows how each attention vector highlights different parts of the name. During hyperparameter tuning, I set <code class="highlighter-rouge">hops</code> as 30 and ended up seeing nearly all the letters highlighted. The model performance was not inferior, but unnecessarily large hops severely damaged the model interpretability.</p>

<h3 id="attention-on-names">Attention on Names</h3>

<p>To make it more visually appealing, I summed up all the row vectors and normalized it (softmax) to overlay on the text directly.</p>

<p>Here are some of the famous names from Happy Potter.</p>

<p><img src="/assets/selfattention/harrypotter.png" alt="harrypotter" /></p>

<p>Apart from Harry, Hermione, Albus, and Draco, attentions tend to locate at the end of the names. It agrees with my previous hypothesis and simple aggregation results.</p>

<p>The following are the characters from Marvel Cinematic Universe.</p>

<p><img src="/assets/selfattention/mcu.png" alt="mcu" /></p>

<p>The model thought Tony and Loki as girly names and pepper as a boy’s name. ‘er’ must be an unusual ending for female names.</p>

<p>How would the model react to the various endings?</p>

<p><img src="/assets/selfattention/variants_of_cat.png" alt="variants_of_cat" /></p>

<p>‘ne’ and ‘na’ endings boost the probability for female.</p>

<p><img src="/assets/selfattention/variants_of_chris.png" alt="variants_of_chris" /></p>

<p>Chris and Christina are classified as female names by the model, but the ending ‘o’ flips the result. Christian is misclassified as a female name.</p>

<p>Albeit some prediction mistakes, the model does do its job. Would it also work decently on the names that it has nearly never seen before? The majority of the names in my dataset have western origins. Only a handful are from South Korea. Let’s see how the model works on my colleagues’ names.</p>

<p><img src="/assets/selfattention/koreanboys.png" alt="koreanboys" /></p>

<p>Boys are all correctly classified.</p>

<p><img src="/assets/selfattention/koreangirls.png" alt="koreangirls" /></p>

<p>Girls are all incorrect. It seems that the typical Korean female name endings are perceived boyish by the model.</p>

<p>Lastly, what about my own name?</p>

<p><img src="/assets/selfattention/variants_of_junsik.png" alt="variants_of_junsik" /></p>

<p>The model predicted Jun as a girly name, but its probability is much lower than June’s. So June did sound female to the neural network because of the <code class="highlighter-rouge">e</code> at the end.</p>

<h3 id="on-embedding-space">On Embedding Space</h3>

<p>During the forward propagation, the model obtains the embedded representation of the input data (name in my case). If the embedding matrices are numerical versions of the input texts, would they be forming clusters based on their semantic meaning and features?</p>

<p>I flattened the embedding matrix into a single vector and reduced its dimensions to 2 using TSNE algorithm. It took too long to process all 30,000 names, so I picked the most popular 100 names from each sex for visualization.</p>

<p>First of all, by sex.</p>

<p><img src="/assets/selfattention/bySex.png" alt="bySex" /></p>

<p>TSNE works wonderfully like in many other cases. Boys’ names form a big cluster on the top right corners and girls’ names bottom left. It seems that the names that are located close to each other tend to end the same.</p>

<p>Would the origins of the names influence how they end?</p>

<p><img src="/assets/selfattention/byOrigin.png" alt="byOrigin" /></p>

<p>It’s not as apparent as sex, but the names tend to cluster by their origins. Grayson, Jackson, Brandon, Jameson from the UK, Valentina, Emilia, Victoria, Aurora, Olivia from Latin heritage.</p>

<p>male by origin-</p>

<p><img src="/assets/selfattention/boysByOrigin.png" alt="boysByOrigin" /></p>

<p>female by origin-</p>

<p><img src="/assets/selfattention/girlsByOrigin.png" alt="girlsByOrigin" /></p>

<h3 id="matrix-computation-emilia---emily--lucy">Matrix Computation: Emilia - Emily + Lucy?</h3>

<p>King - Man + Woman = Queen은 Word Embedding의 멋진 사례 중 하나로 빠짐없이 등장한다. 왕에서 남자라는 성을 제거하면 권력이 남고, 그 권력을 여자라는 성에 더하면 여왕이 된다. 관념적으로도 말이 되고 간단한 사칙연산으로도 말이 된다. 뉴럴 네트워크를 통해 얻은 숫자 뭉치가 우리가 가진 관념적인 정보를 담고 있음을 보여주는 멋진 결과다.</p>

<p><img src="/assets/selfattention/king_to_queen.jpeg" alt="king_to_queen" /></p>

<p>source: https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623</p>

<p>단어와 마찬가지로 이름을 임베딩 스페이스상에 표현할 수 있다면 사칙연산을 통해 재밌는 결과를 만들어볼 수 있을까? 이름은 단어만큼 풍부한 의미를 가지지는 않지만, 그냥 해보자.</p>

<p>먼저 Training과 Validation 데이터셋을 모두 모델에 넣어 이름의 임베딩을 얻었다. 각 임베딩은 <code class="highlighter-rouge">hops</code>x<code class="highlighter-rouge">hidden_dim * 2</code>크기의 매트릭스 형태로 내 실험에서는 5x600 크기였다. 이름 3개를 고른 후 element-wise로 - + 연산을 수행하여 query matrix를 만든다. 그리고 이 query matrix와 모든 embedding matrix간의 matrix euclidean distance를 구하고, distance가 가장 낮은 5개를 출력해보았다.</p>

<p>1) Emilia - Emily  + Lucy = Lucia!</p>

<p><img src="/assets/selfattention/emily.png" alt="emily" /></p>

<p>Emilia에서 Emily를 빼면 ~ia가 남고 거기에 Lucy를 더하면 Lucia가 될거라고 생각했는데 실제로 됐다!</p>

<p>2) Susie - Susanne + Roxie = Roxie!</p>

<p><img src="/assets/selfattention/susie.png" alt="susie" /></p>

<p>3) Christina - Christine + Austine = Austina!</p>

<p><img src="/assets/selfattention/christina.png" alt="christina" /></p>

<p>예상과 같은 결과가 나오긴 하지만 King - Man + Woman = Queen 만큼 어떤 추상적인 의미를 조작했다고 보기는 어렵다. 예를 들어 “Paul”에서 “John”을 뺀 다음 “Hank”를 더하면 어떨까?</p>

<p>4) Paul - John + Hank = ?</p>

<p><img src="/assets/selfattention/paul.png" alt="paul" /></p>

<p>“Bank”라는 결과를 얻을 수 있지만 어떤 로직이나 의미가 느껴지지 않는다.</p>

<h2 id="outro">Outro</h2>

<p>이번 포스팅에서는 LSTM + Self Attention 모델을 사용해 남아 / 여아 이름을 분류하는 모델을 만들고, Attention을 통해 뉴럴 네트워크가 이름의 어떤 부분에 집중했는지 시각화해보았다. 뉴럴 네트워크가 집중했던 흔적은 보통 이름의 끝 부분에 남겨져 있었다. 같은 Jun이라도 e가 붙으면 여자가 되고 sik이 붙으면 남자가 되었다. 분석 전에 내가 어렴풋이 생각했던 가설과 비슷해 놀라웠다. 또 2차원 공간에 뿌려진 이름들이 성별이나 문화권에 따라 끼리끼리 모이는 점도 흥미로웠다. 해석가능한 딥러닝은 내가 생각하는 방식을 되돌아보는 재미가 있다.</p>

<h2 id="reference">Reference</h2>
<p><a href="https://arxiv.org/pdf/1703.03130.pdf">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</a><br />
<a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding">An open-source implementation of the paper ‘A Structured Self-Attentive Sentence Embedding’ published by IBM and MILA.</a></p>

          <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- ad -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4190486040269313"
     data-ad-slot="1008472640"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

        </section>
        <footer class="post-footer">
          <section class="share">
            
              
                <a class="icon-twitter" href="http://twitter.com/share?text=Self+Attention%3A+Name+Classifier&amp;url=http://localhost:4000/python/2018/09/01/name_classifier copy"
                  onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
                <i class="fa fa-twitter"></i><span class="hidden">twitter</span>
                </a>
              
            
              
            
          </section>
        </footer>
        <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Junsik Whang</h4>
              <p class="bio"></p>
              <hr>
              <p class="published">Published <time datetime="2018-09-01 09:00">01 Sep 2018</time></p>
            </section>
          </div>
          
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="/">Junsik Whang</a> &copy; 2018<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div>
      </article>
    </main>
    <div class="bottom-closer">
      <div class="background-closer-image"  style="background-image: url(/assets/images/cA4aKEIPQrerBnp1yGHv_IMG_9534-3-2.jpg)">
        Image
      </div>
      <div class="inner">
        <h1 class="blog-title">jsideas</h1>
        <h2 class="blog-description">a novice's journey into data science
</h2>
        <a href="/" class="btn">Back to Overview</a>
      </div>
    </div>
    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-36651119-2', 'auto');
  ga('send', 'pageview');

</script>

  </body>
</html>
