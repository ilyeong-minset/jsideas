<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>TF: tips 3 - CNN 개념 정리</title>
  <meta name="description" content="a novice's journey into data science
" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <link rel="canonical" href="http://jsideas.net/python/2017/05/02/tf-tips3.markdown.html">

  <link rel="shortcut icon" href="/assets/images/favicon.ico">
<!--  <link rel="stylesheet" href=""> -->
  <link rel="stylesheet" href="http://brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
</head>

  <body itemscope itemtype="http://schema.org/Article">
    <!-- header start -->

<a href="http://jsideas.net" class="logo-readium"><span class="logo" style="background-image: url(/assets/images/df_logo.jpg)"></span></a>

<!-- header end -->

    <main class="content" role="main">
      <article class="post">
        
        <div class="article-image">
          <div class="post-image-image" style="background-image: url(/assets/tf/header.jpg)">
            Article Image
          </div>
          <div class="post-meta">
            <h1 class="post-title">TF: tips 3 - CNN 개념 정리</h1>
            <div class="cf post-meta-text">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person">Junsik Whang</h4>
              on
              <time datetime="2017-05-02 09:00">02 May 2017</time>
              <!-- , tagged on <span class="post-tag-">, <a href="/tag/"></a></span> -->
            </div>
            <div style="text-align:center">
              <a href="#topofpage" class="topofpage"><i class="fa fa-angle-down"></i></a>
            </div>
          </div>
        </div>
        
        <section class="post-content">
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
          <a name="topofpage"></a>
          <h2 id="tf-tips-3---cnn-개념-정리">tf tips. 3 - CNN 개념 정리</h2>

<p><code>Convolutional Neural Network</code> (CNN)은 이미지 분류, 오브젝트 인식 등에서 굉장히 인기있는 알고리즘이다. 일반적인 Deep Neural Network에 비해 이미지 처리에 있어 CNN이 더 좋은 성능을 낸다고 알려져 있는데 그 비법은 인풋 데이터의 처리에 있다. DNN에서 인풋 레이어의 모든 값이 그 다음 레이어의 모든 뉴런에 연결된다면, CNN에서는 <code>filter</code>를 사용해서 서로 인접한 인풋 값이나 뉴런을 그 다음 레이어의 뉴런에 전달한다. 그리고 DNN에서 <code>weight</code>가 서로 다른 값을 가지고 있다면, CNN에서 하나의 필터는 같은 <code>weight</code>를 사용함으로써 위치에 관계없이 특정한 패턴을 탐지할 수 있다. 이러한 구조적인 장점에 힘입어, CNN은 자동으로 유의미한 피쳐를 탐지해낸다.</p>

<p>가장 기본적인 형태의 CNN은 <code>convolution layer</code>와 <code>pooling layer</code>로 저차원에서 고차원의 피쳐를 추출한 후 이를 기반으로 분류를 수행하기 위해 <code>fully connected layer</code>를 거친다. MNIST 데이터셋을 CNN에 넣고 돌려보자.</p>

<pre><code class="language-python">import matplotlib.pyplot as plt
from collections import namedtuple
import pandas as pd
%matplotlib inline
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
</code></pre>

<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</code></pre>

<pre><code class="language-python">def build_cnn(learning_rate):
    
    ## define input
    x = tf.placeholder(tf.float32, shape=[None, 784])
    y_ = tf.placeholder(tf.float32, shape=[None, 10])

    ## weight and bias variables
    def weight_variable(shape):
        initial = tf.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial)

    def bias_variable(shape):
        initial = tf.constant(0.1, shape=shape)
        return tf.Variable(initial)
    
    ## define conv and max_pool
    def conv2d(x, W):
        return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')

    def max_pool_2x2(x):
        return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')

    ## define operation
    ### conv1: depth 32
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])

    x_image = tf.reshape(x, [-1, 28, 28, 1])
    conv1 = conv2d(x_image, W_conv1) + b_conv1
    h_conv1 = tf.nn.relu(conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    layer1 = h_pool1
    
    ### conv1: depth 64
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])

    conv2 = conv2d(layer1, W_conv2) + b_conv2
    h_conv2 = tf.nn.relu(conv2)
    h_pool2 = max_pool_2x2(h_conv2)
    layer2 = h_pool2
    
    ### fc1: 1024
    W_fc1 = weight_variable([7*7*64, 1024])
    b_fc1 = bias_variable([1024])
    layer2_matrix = tf.reshape(layer2, [-1, 7*7*64])
    matmul_fc1 = tf.matmul(layer2_matrix, W_fc1) + b_fc1
    h_fc1 = tf.nn.relu(matmul_fc1)
    layer3 = h_fc1

    ### dropout on fc1
    keep_prob = tf.placeholder(tf.float32)
    layer3_drop = tf.nn.dropout(layer3, keep_prob)

    ### fc2: 10
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    matmul_fc2 = tf.matmul(layer3_drop, W_fc2) + b_fc2
    y_conv = tf.nn.softmax(matmul_fc2)
    layer4 = y_conv
    
    ### cross_entropy and train_step
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=layer4)
    cost = tf.reduce_mean(cross_entropy)
    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)
    
    ### prediction and accruacy
    correct_prediction = tf.equal(tf.argmax(layer4, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    
    ### graph nodes
    export_nodes = ['x', 'y_', 'keep_prob', 'cost', 'correct_prediction', 'accuracy', 'train_step']
    Graph = namedtuple('Graph', export_nodes)
    local_dict = locals()
    graph = Graph(*[local_dict[each] for each in export_nodes])
    
    return graph
</code></pre>

<pre><code class="language-python">## hyperparams
batch_size = 100
epochs = 2000
learning_rate = 1e-4
keep_prob = 0.5
</code></pre>

<pre><code class="language-python">## run operation
model = build_cnn(learning_rate)
cnn_train_cost = []
cnn_val_acc = []

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for e in range(epochs):
        batch = mnist.train.next_batch(batch_size)
        
        if (e % 100 == 0) | (e == epochs-1):
            train_acc = sess.run(model.accuracy, feed_dict={
                model.x: batch[0], model.y_: batch[1], model.keep_prob: 1.0
            })
            print("step %d, training_accuracy %g" %(e, train_acc))
            
            val_acc = sess.run(model.accuracy, feed_dict={
                model.x: mnist.test.images, model.y_: mnist.test.labels, model.keep_prob: 1.0
            })
            print("test_accuracy %g" % val_acc)
            cnn_val_acc.append({'e': e, 'val_acc': val_acc})
        _, t_cost = sess.run([model.train_step, model.cost], feed_dict={model.x: batch[0], model.y_: batch[1], model.keep_prob: keep_prob})
        cnn_train_cost.append({'e': e, 'cost': t_cost})
    
</code></pre>

<pre><code>step 0, training_accuracy 0.13
test_accuracy 0.1251
step 100, training_accuracy 0.77
test_accuracy 0.7613
step 200, training_accuracy 0.92
test_accuracy 0.9033
step 300, training_accuracy 0.93
test_accuracy 0.9255
step 400, training_accuracy 0.94
test_accuracy 0.9392
step 500, training_accuracy 0.91
test_accuracy 0.9468
step 600, training_accuracy 0.94
test_accuracy 0.9503
step 700, training_accuracy 0.99
test_accuracy 0.9532
step 800, training_accuracy 0.98
test_accuracy 0.9552
step 900, training_accuracy 0.93
test_accuracy 0.9622
step 1000, training_accuracy 0.96
test_accuracy 0.9634
step 1100, training_accuracy 0.98
test_accuracy 0.9664
step 1200, training_accuracy 0.95
test_accuracy 0.9651
step 1300, training_accuracy 0.97
test_accuracy 0.9693
step 1400, training_accuracy 0.96
test_accuracy 0.9717
step 1500, training_accuracy 0.99
test_accuracy 0.9703
step 1600, training_accuracy 0.94
test_accuracy 0.9734
step 1700, training_accuracy 0.98
test_accuracy 0.9722
step 1800, training_accuracy 0.95
test_accuracy 0.9738
step 1900, training_accuracy 0.97
test_accuracy 0.9758
step 1999, training_accuracy 0.98
test_accuracy 0.9771
</code></pre>

<pre><code class="language-python">cnn_train_df = pd.DataFrame.from_dict(cnn_train_cost)
cnn_train_df.set_index('e', inplace=True)

cnn_val_df = pd.DataFrame.from_dict(cnn_val_acc)
cnn_val_df.set_index('e', inplace=True)

plt.plot(cnn_train_df, label="train_cost")
plt.plot(cnn_val_df, label="val_acc")
plt.legend()
plt.show()
</code></pre>

<p><img src="/assets/CNN_tutorial_files/CNN_tutorial_7_0.png" alt="png" /></p>

<h2 id="비교를-위해-간단한-dnn을-만든다">비교를 위해 간단한 DNN을 만든다</h2>

<p>CNN의 <code>convolution layer</code> 뒤에 붙은 <code>fully connected layer</code>를 따로 떼어 DNN으로 만들어 같은 하이퍼파라미터로 학습시킨다.</p>

<pre><code class="language-python">def build_dnn(learning_rate):
    
    ## define input
    x = tf.placeholder(tf.float32, shape=[None, 784])
    y_ = tf.placeholder(tf.float32, shape=[None, 10])

    
    ### fc1: 1024
    W_fc1 = weight_variable([784, 1024])
    b_fc1 = bias_variable([1024])
    layer2_matrix = tf.reshape(x, [-1, 784])
    matmul_fc1 = tf.matmul(layer2_matrix, W_fc1) + b_fc1
    h_fc1 = tf.nn.relu(matmul_fc1)
    layer3 = h_fc1

    ### dropout on fc1
    keep_prob = tf.placeholder(tf.float32)
    layer3_drop = tf.nn.dropout(layer3, keep_prob)

    ### fc2: 10
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    matmul_fc2 = tf.matmul(layer3_drop, W_fc2) + b_fc2
    y_conv = tf.nn.softmax(matmul_fc2)
    layer4 = y_conv
    
    ### cross_entropy and train_step
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=layer4)
    cost = tf.reduce_mean(cross_entropy)
    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)
    
    ### prediction and accruacy
    correct_prediction = tf.equal(tf.argmax(layer4, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    
    ### graph nodes
    export_nodes = ['x', 'y_', 'keep_prob', 'cost', 'correct_prediction', 'accuracy', 'train_step']
    Graph = namedtuple('Graph', export_nodes)
    local_dict = locals()
    graph = Graph(*[local_dict[each] for each in export_nodes])
    
    return graph
</code></pre>

<pre><code class="language-python">## run operation
model = build_dnn(learning_rate)
dnn_train_cost = []
dnn_val_acc = []

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    for e in range(epochs):
        batch = mnist.train.next_batch(batch_size)
        
        if (e % 100 == 0) | (e == epochs-1):
            train_acc = sess.run(model.accuracy, feed_dict={
                model.x: batch[0], model.y_: batch[1], model.keep_prob: 1.0
            })
            print("step %d, training_accuracy %g" %(e, train_acc))
            
            val_acc = sess.run(model.accuracy, feed_dict={
                model.x: mnist.test.images, model.y_: mnist.test.labels, model.keep_prob: 1.0
            })
            print("test_accuracy %g" % val_acc)
            dnn_val_acc.append({'e': e, 'val_acc': val_acc})
        _, t_cost = sess.run([model.train_step, model.cost], feed_dict={model.x: batch[0], model.y_: batch[1], model.keep_prob: keep_prob})
        dnn_train_cost.append({'e': e, 'cost': t_cost})
    
</code></pre>

<pre><code>step 0, training_accuracy 0.11
test_accuracy 0.1089
step 100, training_accuracy 0.59
test_accuracy 0.6115
step 200, training_accuracy 0.62
test_accuracy 0.7274
step 300, training_accuracy 0.84
test_accuracy 0.7722
step 400, training_accuracy 0.84
test_accuracy 0.8707
step 500, training_accuracy 0.93
test_accuracy 0.9026
step 600, training_accuracy 0.92
test_accuracy 0.9079
step 700, training_accuracy 0.87
test_accuracy 0.9121
step 800, training_accuracy 0.93
test_accuracy 0.9138
step 900, training_accuracy 0.9
test_accuracy 0.9208
step 1000, training_accuracy 0.99
test_accuracy 0.9218
step 1100, training_accuracy 0.93
test_accuracy 0.924
step 1200, training_accuracy 0.91
test_accuracy 0.929
step 1300, training_accuracy 0.96
test_accuracy 0.9303
step 1400, training_accuracy 0.96
test_accuracy 0.9297
step 1500, training_accuracy 0.95
test_accuracy 0.9317
step 1600, training_accuracy 0.93
test_accuracy 0.9331
step 1700, training_accuracy 0.93
test_accuracy 0.9342
step 1800, training_accuracy 0.96
test_accuracy 0.9373
step 1900, training_accuracy 0.91
test_accuracy 0.9379
step 1999, training_accuracy 0.95
test_accuracy 0.9388
</code></pre>

<pre><code class="language-python">dnn_train_df = pd.DataFrame.from_dict(dnn_train_cost)
dnn_train_df.set_index('e', inplace=True)

dnn_val_df = pd.DataFrame.from_dict(dnn_val_acc)
dnn_val_df.set_index('e', inplace=True)

plt.plot(dnn_train_df, label="train_cost")
plt.plot(dnn_val_df, label="val_acc")
plt.legend()
plt.show()
</code></pre>

<p><img src="/assets/CNN_tutorial_files/CNN_tutorial_11_0.png" alt="png" /></p>

<p>CNN 만큼은 아니지만 DNN도 꽤 괜찮은 성능을 보였다. 모델간 성능 비교를 위해 <code>train_cost</code>와 <code>val_acc</code>를 플롯화해보자.</p>

<pre><code class="language-python">plt.plot(cnn_train_df, label="cnn_train_cost", alpha=0.5)
plt.plot(dnn_train_df, label="dnn_train_cost", alpha=0.5)
plt.legend()
plt.show()
</code></pre>

<p><img src="/assets/CNN_tutorial_files/CNN_tutorial_13_0.png" alt="png" /></p>

<pre><code class="language-python">plt.plot(cnn_val_df, label="cnn_val_acc", alpha=0.5)
plt.plot(dnn_val_df, label="dnn_val_acc", alpha=0.5)
plt.legend()
plt.show()
</code></pre>

<p><img src="/assets/CNN_tutorial_files/CNN_tutorial_14_0.png" alt="png" /></p>

<p>CNN이 DNN에 비해 <code>train_cost</code>가 더 빨리 떨어지며, <code>validation accuracy</code> 역시 빠르게 상승하는 것을 볼 수 있다. 또한 학습을 시킴에 따라 <code>validation_accuracy</code>가 두 모델 다 수렴했으나, CNN의 수렴점이 DNN의 그것에 비해 소폭 높음을 확인할 수 있다.</p>


        </section>
        <footer class="post-footer">
          <section class="share">
            
              
                <a class="icon-twitter" href="http://twitter.com/share?text=TF%3A+tips+3+-+CNN+%EA%B0%9C%EB%85%90+%EC%A0%95%EB%A6%AC&amp;url=http://jsideas.net/python/2017/05/02/tf-tips3.markdown"
                  onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
                <i class="fa fa-twitter"></i><span class="hidden">twitter</span>
                </a>
              
            
              
            
          </section>
        </footer>
        <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Junsik Whang</h4>
              <p class="bio"></p>
              <hr>
              <p class="published">Published <time datetime="2017-05-02 09:00">02 May 2017</time></p>
            </section>
          </div>
          
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="/">Junsik Whang</a> &copy; 2018<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div>
      </article>
    </main>
    <div class="bottom-closer">
      <div class="background-closer-image"  style="background-image: url(/assets/images/cA4aKEIPQrerBnp1yGHv_IMG_9534-3-2.jpg)">
        Image
      </div>
      <div class="inner">
        <h1 class="blog-title">jsideas</h1>
        <h2 class="blog-description">a novice's journey into data science
</h2>
        <a href="/" class="btn">Back to Overview</a>
      </div>
    </div>
    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-36651119-2', 'auto');
  ga('send', 'pageview');

</script>

  </body>
</html>
