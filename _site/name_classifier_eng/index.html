<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Self Attention: Name Classifier - jsideas</title>


  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="jsideas" property="og:site_name">
  
    <meta content="Self Attention: Name Classifier" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="a novice's journey into data science
" property="og:description">
  
  
    <meta content="http://localhost:4000/name_classifier_eng/" property="og:url">
  
  
    <meta content="2018-09-01T09:00:00+09:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/20180901.png" property="og:image">
  
  
    
  
  
    
    <meta content="python" property="article:tag">
    
    <meta content="deep learning" property="article:tag">
    
    <meta content="nlp" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@junsik_whang">
  
    <meta name="twitter:title" content="Self Attention: Name Classifier">
  
  
    <meta name="twitter:url" content="http://localhost:4000/name_classifier_eng/">
  
  
    <meta name="twitter:description" content="a novice's journey into data science
">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/20180901.png">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<!-- <link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon"> -->
	<!-- <link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png"> -->
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/author.jpg" alt="Junsik Hwang"></a>
      </div>
      <div class="author-name">Junsik Hwang</div>
      <p>I do data analytics and modelling for a living and for fun</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/junsik_whang" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/junkwhinger" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/jswhang" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:junsik.whang@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2019 &copy; Junsik Hwang</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/20180901.png alt="Self Attention: Name Classifier">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Self Attention: Name Classifier</h1>
        <div class="page-date"><span>2018, Sep 01&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p>On hearing a person’s name, we can often correctly tell if it’s he or she. Male names tend to have strong pronunciations like Mark, Robert, and Lucas. On the other hand, female names are likely to sound smoother like Lucy, Stella, and Valerie. Would a neural network be able to replicate our classification process? And what part of names would it pay attention to mainly?</p>

<p><br /></p>

<h1 id="dataset">Dataset</h1>

<p>For this personal research project, I crawled commonly used baby names that are freely available on the internet. Some of them had metadata like origin and popularities.</p>

<h2 id="example">example</h2>

<table>
  <thead>
    <tr>
      <th>babyname</th>
      <th>sex</th>
      <th>origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Aakesh</td>
      <td>boy</td>
      <td>Indian</td>
    </tr>
    <tr>
      <td>Aaren</td>
      <td>boy</td>
      <td>Hebrew</td>
    </tr>
    <tr>
      <td>Abalina</td>
      <td>girl</td>
      <td>Hebrew</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="exploration">Exploration</h2>

<p>Let’s dive into the dataset and find out some useful patterns.</p>

<p><br /></p>

<h3 id="most-frequently-used-first-letter">Most frequently used first letter</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>A - 3,792 (10.2%)</td>
      <td>A - 2,118 (9.7%)</td>
      <td>A - 1,674 (10.8%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>C - 2,887 (7.7%)</td>
      <td>C - 1,880 (8.6%)</td>
      <td>B - 1,051 (6.8%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>S - 2,862 (7.7%)</td>
      <td>S - 1,825 (8.4%)</td>
      <td>M - 1,051 (6.8%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>M - 2,615 (7.0%)</td>
      <td>M - 1,564 (7.2%)</td>
      <td>S - 1,037 (6.7%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>K - 2,262 (6.1%)</td>
      <td>K - 1,531 (7.0%)</td>
      <td>C - 1,007 (6.5%)</td>
    </tr>
  </tbody>
</table>

<p>A is the most commonly chosen first letter in both sexes. B stands out among the boys’ first letters.</p>

<p><br /></p>

<h3 id="most-frequently-used-first-two-letters">Most frequently used first two letters</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Ma - 1,454 (3.9%)</td>
      <td>Ma - 907 (4.2%)</td>
      <td>Ma - 547 (3.5%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Ka - 914 (2.4%)</td>
      <td>Ka - 726 (3.3%)</td>
      <td>Ha - 368 (2.4%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Sh - 864 (2.3%)</td>
      <td>Sh - 715 (3.3%)</td>
      <td>Al - 336 (2.2%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Ca - 856 (2.3%)</td>
      <td>Ca - 624 (2.9%)</td>
      <td>Da - 305 (2.0%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Al - 842 (2.3%)</td>
      <td>Ch - 600 (2.8%)</td>
      <td>De - 300 (1.9%)</td>
    </tr>
  </tbody>
</table>

<p>M pops up in the top first two letters. The difference between sexes is a little bit more prominent than the first letter chart.</p>

<p><br /></p>

<h3 id="most-dominant-first-two-letters-by-sex">Most dominant first two letters by sex</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>first_two_letters</th>
      <th>Girl_ratio</th>
      <th>Boy_ratio</th>
      <th>Absolute Difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Sh</td>
      <td>3.3%</td>
      <td>1.0%</td>
      <td>2.3%</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Ka</td>
      <td>3.3%</td>
      <td>1.2%</td>
      <td>2.1%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Ch</td>
      <td>2.8%</td>
      <td>1.3%</td>
      <td>1.5%</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Ha</td>
      <td>0.9%</td>
      <td>2.4%</td>
      <td>1.5%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Ca</td>
      <td>2.9%</td>
      <td>1.5%</td>
      <td>1.4%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Ba</td>
      <td>0.5%</td>
      <td>1.7%</td>
      <td>1.2%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>Ga</td>
      <td>0.4%</td>
      <td>1.4%</td>
      <td>1.0%</td>
    </tr>
    <tr>
      <td>8</td>
      <td>Jo</td>
      <td>1.8%</td>
      <td>1.0%</td>
      <td>0.8%</td>
    </tr>
    <tr>
      <td>9</td>
      <td>La</td>
      <td>2.0%</td>
      <td>1.3%</td>
      <td>0.7%</td>
    </tr>
    <tr>
      <td>10</td>
      <td>Co</td>
      <td>0.9%</td>
      <td>1.6%</td>
      <td>0.7%</td>
    </tr>
  </tbody>
</table>

<p>Sh, Ka, Ch, La, Jo are used more frequently in female names than male names. Male names prefer Ha, Ba, Ga, Co as their opening sequences. But the ratio gaps don’t seem to be widened significantly.</p>

<p><br /></p>

<h3 id="most-frequently-used-last-letter">Most frequently used last letter</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>a - 10,693 (28.6%)</td>
      <td>a - 10,198 (46.8%)</td>
      <td>n - 3,123 (20.1%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>e - 6,471 (17.3%)</td>
      <td>e - 4,989 (22.9%)</td>
      <td>o - 1,519 (9.8%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>n - 4,813 (12.9%)</td>
      <td>n - 1,690 (7.8%)</td>
      <td>e - 1,482 (9.5%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>y - 1,992 (5.3%)</td>
      <td>y - 1,036 (4.8%)</td>
      <td>s - 1,448 (9.3%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>s - 1,825 (4.9%)</td>
      <td>i - 927 (4.3%)</td>
      <td>r - 1,123 (7.2%)</td>
    </tr>
  </tbody>
</table>

<p>The last letter might hold some clues.
Nearly half of the female names end with A. A fifth of them have E ending. These endings are not as dominant in male names as in female names.</p>

<p><br /></p>

<h3 id="most-dominant-last-letter-by-sex">Most dominant last letter by sex</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>last_letter</th>
      <th>Girl_ratio</th>
      <th>Boy_ratio</th>
      <th>Absolute Difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>a</td>
      <td>46.8%</td>
      <td>3.2%</td>
      <td>43.6%</td>
    </tr>
    <tr>
      <td>2</td>
      <td>e</td>
      <td>22.9%</td>
      <td>9.5%</td>
      <td>13.4%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>n</td>
      <td>7.8%</td>
      <td>20.1%</td>
      <td>12.3%</td>
    </tr>
    <tr>
      <td>4</td>
      <td>o</td>
      <td>0.8%</td>
      <td>9.8%</td>
      <td>9.0%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>s</td>
      <td>1.7%</td>
      <td>9.3%</td>
      <td>7.6%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>r</td>
      <td>0.9%</td>
      <td>7.2%</td>
      <td>6.3%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>d</td>
      <td>0.7%</td>
      <td>5.9%</td>
      <td>5.2%</td>
    </tr>
    <tr>
      <td>8</td>
      <td>l</td>
      <td>2.6%</td>
      <td>6.6%</td>
      <td>4.0%</td>
    </tr>
    <tr>
      <td>9</td>
      <td>k</td>
      <td>0.1%</td>
      <td>2.7%</td>
      <td>2.6%</td>
    </tr>
    <tr>
      <td>10</td>
      <td>t</td>
      <td>1.5%</td>
      <td>4.0%</td>
      <td>2.5%</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="most-frequently-used-last-two-letters">Most frequently used last two letters</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>Total</th>
      <th>Girl</th>
      <th>Boy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>na - 2,419 (6.5%)</td>
      <td>na - 2,375 (10.9%)</td>
      <td>on - 890 (5.7%)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>ne - 1,713 (4.6%)</td>
      <td>ne - 1,476 (6.8%)</td>
      <td>an - 763 (4.9%)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ia - 1,510 (4.0%)</td>
      <td>ia - 1,472 (6.8%)</td>
      <td>er - 558 (3.6%)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>ie - 1,022 (2.7%)</td>
      <td>la - 909 (4.2%)</td>
      <td>in - 482 (3.1%)</td>
    </tr>
    <tr>
      <td>5</td>
      <td>on - 1,000 (2.7%)</td>
      <td>ta - 837 (3.8%)</td>
      <td>us - 476 (3.1%)</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="most-dominant-last-two-letters-by-sex">Most dominant last two letters by sex</h3>

<table>
  <thead>
    <tr>
      <th>Rank</th>
      <th>last_letter</th>
      <th>Girl_ratio</th>
      <th>Boy_ratio</th>
      <th>Absolute Difference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>na</td>
      <td>10.9%</td>
      <td>0.3%</td>
      <td>10.6%</td>
    </tr>
    <tr>
      <td>2</td>
      <td>ia</td>
      <td>6.8%</td>
      <td>0.2%</td>
      <td>6.6%</td>
    </tr>
    <tr>
      <td>3</td>
      <td>ne</td>
      <td>6.8%</td>
      <td>1.5%</td>
      <td>5.3%</td>
    </tr>
    <tr>
      <td>4</td>
      <td>on</td>
      <td>0.5%</td>
      <td>5.7%</td>
      <td>5.2%</td>
    </tr>
    <tr>
      <td>5</td>
      <td>la</td>
      <td>4.2%</td>
      <td>0.2%</td>
      <td>4.0%</td>
    </tr>
    <tr>
      <td>6</td>
      <td>an</td>
      <td>0.9%</td>
      <td>4.9%</td>
      <td>4.0%</td>
    </tr>
    <tr>
      <td>7</td>
      <td>ta</td>
      <td>3.8%</td>
      <td>0.2%</td>
      <td>3.6%</td>
    </tr>
    <tr>
      <td>8</td>
      <td>ra</td>
      <td>3.6%</td>
      <td>0.2%</td>
      <td>3.4%</td>
    </tr>
    <tr>
      <td>9</td>
      <td>er</td>
      <td>0.4%</td>
      <td>3.6%</td>
      <td>3.2%</td>
    </tr>
    <tr>
      <td>10</td>
      <td>da</td>
      <td>3.3%</td>
      <td>0.1%</td>
      <td>3.2%</td>
    </tr>
  </tbody>
</table>

<p>The simple aggregations above revealed that the last one or two letters might influence how we tell male names from female names. Would a neural classifier perform better than simple rules? Would it also pay attention to how the names end?</p>

<p><br /></p>

<h1 id="model">Model</h1>

<p>Text classification models usually extract patterns from text via RNN or CNN layers before processing them through a single or multiple fully connected layers to obtain probability values for each class.</p>

<p><img src="/assets/materials/20180901/basic_lstm_classifier.png" alt="basic_lstm_classifier" height="600" /></p>

<p>BiLSTM model above does perform well, but it does not tell us the evidence that it takes into account. I could use occlusion methods like Local Interpretable Model-Agnostic Explanations(LIME), but these methods run the model multiple times with masked inputs to figure out the part that contributes the most. There must be a more straight-forward and convenient way to do this.</p>

<p><br /></p>

<h2 id="a-structure-self-attentive-sentence-embedding">A Structure Self-Attentive Sentence Embedding</h2>

<p>A Structured Self-Attentive Sentence Embedding (2017.03) introduces Self-Attention to instill visibility into the text model.</p>

<p><img src="/assets/materials/20180901/self_attention.png" alt="self_attention" height="600" /></p>

<p>The self-attention model uses the same BiLSTM feature extractor as an ordinary text classifier. It passes the feature through two fully connected layers (W_s1, W_s2) to achieve Attention matrix whose shape is n_token x hops. <code class="highlighter-rouge">da</code> for W_s1 and <code class="highlighter-rouge">hops</code> for W_s2 are hyperparameters. Compared to the conventional attention mechanisms that output an attention vector, the attention matrix has the following benefits:</p>
<ul>
  <li>Multiple attention vectors represent multiple features that a sentence has.</li>
  <li>The model does not need extra inputs to obtain attention.</li>
  <li>Attention alleviates the burden of LSTM as it accesses all the time steps of the LSTM layer.</li>
  <li>Attention matrix is incredibly easy to visualize as a heat map whose size is hops x n_token.</li>
</ul>

<p>A softmax operation is included in the attention part to normalize each vector so that the visualized heat map would be understood intuitively.</p>

<p>To obtain the classification result, the model multiplies the LSTM output with the attention matrix before flattening and fully connected layers. Some pruning methods are suggested by the paper to decrease the trainable parameters, but I skipped this part as my model itself was not big enough to be pruned.</p>

<p><br /></p>

<h2 id="loss-function">Loss Function</h2>

<p>In addition to the cross-entropy loss for classification error, the paper introduces another loss function called <code class="highlighter-rouge">Penalization Term</code>. The attention matrix is composed of multiple vectors that focus on specific parts of the input sentence. It will be of a huge waste if several attention vectors end up looking at the same area. Penalization term prevents this from happening.</p>

<script type="math/tex; mode=display">P = ||(AA^T - I )||_F^2</script>

<p>The attention matrix <script type="math/tex">A</script> is multiplied with its own transposed matrix before subtracting an identity matrix. The penalization term is the Frobenius norm of the operation.</p>

<p><br /></p>

<h1 id="experiment">Experiment</h1>

<h2 id="training-setting">Training Setting</h2>

<p>The authors carried out some interesting experiments such as classifying emotions of reviews and predicting ages of Twitterians by their tweets. The fundamental component of the inputs used in these experiments is words.</p>

<p>However, in my experiment, the basic unit is alphabets. As the diversity of alphabet letters is far less complicated than of English word I thought the hyperparameters used in the paper would be much of an overkill to classify names.</p>

<p>So I reduced some of the hyperparameters to 1/10 or 1/5 of the ones suggested by the paper.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "num_epochs": 10,
    "batch_size": 16,
    "save_summary_steps": 100,
    "learning_rate": 0.001,
    "weight_decay": 0.0001,
    "embedding_dim": 100,
    "hidden_dim": 300,
    "nb_layers": 1,
    "nb_hops": 5,
    "da": 30,
    "fc_ch": 300,
    "nb_classes": 2,
    "device": "cpu",
    "train_size": 33609,
    "val_size": 3735,
    "vocab_size": 35,
    "coef": 0.5,
    "isPenalize": 1,
    "dropout": 0.0,
    "model": "selfattention"
}
</code></pre></div></div>

<p><br /></p>

<h2 id="model-performance">Model Performance</h2>
<p>For classification performance evaluation I made a baseline BiLSTM + MaxPooling model that uses the same hyperparameter settings as the one with self-attention.</p>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>Validation Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BiLSTM + MaxPooling (epoch 8)</td>
      <td>0.884</td>
    </tr>
    <tr>
      <td>Self Attention (epoch 10)</td>
      <td>0.892</td>
    </tr>
  </tbody>
</table>

<p>In line with the experimental results of the paper, the self-attention model outperformed the baseline model regarding validation accuracy. The training accuracy was about the same, indicating that the model learned generalized representations.</p>

<p>By class…</p>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>Precision(girl)</th>
      <th>Precision(boy)</th>
      <th>Recall(girl)</th>
      <th>Recall(boy)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BiLSTM + MaxPooling</td>
      <td>0.901</td>
      <td>0.860</td>
      <td>0.900</td>
      <td>0.862</td>
    </tr>
    <tr>
      <td>Self Attention</td>
      <td>0.903</td>
      <td>0.876</td>
      <td>0.913</td>
      <td>0.862</td>
    </tr>
  </tbody>
</table>

<p>Neither of them produced biased predictions.</p>

<p><br /></p>

<h1 id="visualizing-self-attention">Visualizing Self Attention</h1>

<h2 id="attention-heatmap">Attention Heatmap</h2>

<p>And here comes the hidden purpose of this blog post. Which part of the names did the model pay attention to before making the final decision? As explained above, each row vector of the attention matrix sums up to 1. All I have to do is to pass the numpy array to matplotlib.</p>

<p><img src="/assets/materials/20180901/attention_heatmap.png" alt="attention_heatmap" /></p>

<p>The heat map above shows how each attention vector highlights different parts of the name. During hyperparameter tuning, I set <code class="highlighter-rouge">hops</code> as 30 and ended up seeing nearly all the letters highlighted. The model performance was not inferior, but unnecessarily large hops severely damaged the model interpretability.</p>

<p><br /></p>

<h2 id="attention-on-names">Attention on Names</h2>

<p>To make it more visually appealing, I summed up all the row vectors and normalized it (softmax) to overlay on the text directly.</p>

<p>Here are some of the famous names from Happy Potter.</p>

<p><img src="/assets/materials/20180901/harrypotter.png" alt="harrypotter" /></p>

<p>Apart from Harry, Hermione, Albus, and Draco, attentions tend to locate at the end of the names. It agrees with my previous hypothesis and simple aggregation results.</p>

<p>The following are the characters from Marvel Cinematic Universe.</p>

<p><img src="/assets/materials/20180901/mcu.png" alt="mcu" /></p>

<p>The model thought Tony and Loki as girly names and pepper as a boy’s name. ‘er’ must be an unusual ending for female names.</p>

<p>How would the model react to the various endings?</p>

<p><img src="/assets/materials/20180901/variants_of_cat.png" alt="variants_of_cat" /></p>

<p>‘ne’ and ‘na’ endings boost the probability for female.</p>

<p><img src="/assets/materials/20180901/variants_of_chris.png" alt="variants_of_chris" /></p>

<p>Chris and Christina are classified as female names by the model, but the ending ‘o’ flips the result. Christian is misclassified as a female name.</p>

<p>Albeit some prediction mistakes, the model does do its job. Would it also work decently on the names that it has nearly never seen before? The majority of the names in my dataset have western origins. Only a handful are from South Korea. Let’s see how the model works on my colleagues’ names.</p>

<p><img src="/assets/materials/20180901/koreanboys.png" alt="koreanboys" /></p>

<p>Boys are all correctly classified.</p>

<p><img src="/assets/materials/20180901/koreangirls.png" alt="koreangirls" /></p>

<p>Girls are all incorrect. It seems that the typical Korean female name endings are perceived boyish by the model.</p>

<p>Lastly, what about my own name?</p>

<p><img src="/assets/materials/20180901/variants_of_junsik.png" alt="variants_of_junsik" /></p>

<p>The model predicted Jun as a girly name, but its probability is much lower than June’s. So June did sound female to the neural network because of the <code class="highlighter-rouge">e</code> at the end.</p>

<p><br /></p>

<h2 id="on-embedding-space">On Embedding Space</h2>

<p>During the forward propagation, the model obtains the embedded representation of the input data (name in my case). If the embedding matrices are numerical versions of the input texts, would they be forming clusters based on their semantic meaning and features?</p>

<p>I flattened the embedding matrix into a single vector and reduced its dimensions to 2 using TSNE algorithm. It took too long to process all 30,000 names, so I picked the most popular 100 names from each sex for visualization.</p>

<p>First of all, by sex.</p>

<p><img src="/assets/materials/20180901/bySex.png" alt="bySex" /></p>

<p>TSNE works wonderfully like in many other cases. Boys’ names form a big cluster on the top right corners and girls’ names bottom left. It seems that the names that are located close to each other tend to end the same.</p>

<p>Would the origins of the names influence how they end?</p>

<p><img src="/assets/materials/20180901/byOrigin.png" alt="byOrigin" /></p>

<p>It’s not as apparent as sex, but the names tend to cluster by their origins. Grayson, Jackson, Brandon, Jameson from the UK, Valentina, Emilia, Victoria, Aurora, Olivia from Latin heritage.</p>

<p>male by origin-</p>

<p><img src="/assets/materials/20180901/boysByOrigin.png" alt="boysByOrigin" /></p>

<p>female by origin-</p>

<p><img src="/assets/materials/20180901/girlsByOrigin.png" alt="girlsByOrigin" /></p>

<p><br /></p>

<h2 id="matrix-computation-emilia---emily--lucy">Matrix Computation: Emilia - Emily + Lucy?</h2>

<p>King - Man + Woman = Queen is a textbook example of word embedding. Not only does it makes semantic sense, but it also satisfies numerical sense.</p>

<p><img src="/assets/materials/20180901/king_to_queen.jpeg" alt="king_to_queen" /><br />
source: <a href="https://medium.com/@thoszymkowiak/how-to-implement-sentiment-analysis-using-word-embedding-and-convolutional-neural-networks-on-keras-163197aef623">medium.com/@thoszymkowiak</a></p>

<p>If the name texts could be expressed in a numerical form, would it also make some interesting numerical computations like King and Queen? 
The semantic meanings of the name text are nowhere near as rich as the English word, but why not? Let’s do it for fun.</p>

<p>I put all the names from the training and validation dataset to obtain name-embedding dictionary. Each embedding matrix is of size hops(5) x 2 hidden_dim(600).
I picked three names to run element-wise plus(+) and minus(-) operation to get the query matrix. 
Then I calculated the matrix euclidean distance between the query matrix and all the embedding matrices listed in the dictionary. 
Then I printed the 5 names with the lowest distance.</p>

<p>1) Emilia - Emily  + Lucy = Lucia!</p>

<p><img src="/assets/materials/20180901/emily.png" alt="emily" /></p>

<p>Emilia - Emily gives me ‘ia’. Adding Lucy to ‘ia’ gives me Lucia!</p>

<p>2) Susie - Susanne + Roxie = Roxie!</p>

<p><img src="/assets/materials/20180901/susie.png" alt="susie" /></p>

<p>3) Christina - Christine + Austine = Austina!</p>

<p><img src="/assets/materials/20180901/christina.png" alt="christina" /></p>

<p>The simple computation does output expected results but it’s not as amazing as King - Man + Woman = Queen. It hardly contains any high-level symantic meanings. Let’s say we subtract “John” from “Paul” and add “Hank”.</p>

<p>4) Paul - John + Hank = ?</p>

<p><img src="/assets/materials/20180901/paul.png" alt="paul" /></p>

<p>We get “Bank” but does it mean anything meaningful?</p>

<p><br /></p>

<h1 id="outro">Outro</h1>

<p>In this simple research project, I implemented LSTM + Self Attention model to classify boy/girl names and visualized where the model paid attention. In many example cases, the model tends to pay special attention to how the names end. ‘e’ at the end makes ‘Jun’ female, but ‘sik’ makes it masculine. It coincides more or less with how I predict if it’s he or she on hearing a person’s name. Interpretable deep learning makes me think about how I think.</p>

<p>My pytorch implementation is on my <a href="https://github.com/junkwhinger/babynames_classifier">github repo</a>.</p>

<p><br /></p>

<h1 id="reference">Reference</h1>
<ul>
  <li><a href="https://arxiv.org/pdf/1703.03130.pdf">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</a></li>
  <li><a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding">An open-source implementation of the paper ‘A Structured Self-Attentive Sentence Embedding’ published by IBM and MILA.</a></li>
</ul>


      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Self Attention: Name Classifier&url=http://localhost:4000/name_classifier_eng/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/name_classifier_eng/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/name_classifier_eng/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/tags#python" class="tag">&#35; python</a>
          
            <a href="/tags#deep learning" class="tag">&#35; deep learning</a>
          
            <a href="/tags#nlp" class="tag">&#35; nlp</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
    <div id="disqus_thread" class="article-comments"></div>
    <script>
      (function() {
          var d = document, s = d.createElement('script');
          s.src = '//jsideas.disqus.com/embed.js';
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
<<<<<<< HEAD
  <!-- <script>
=======
  <script>
>>>>>>> 4dafbe76142890b8168a7a62b37d5596444a3f4e
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-36651119-2', 'auto');
  ga('send', 'pageview');

</script>
<<<<<<< HEAD
 -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-36651119-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-36651119-2', { 'optimize_id': 'GTM-T87V6B5'});
</script>
=======
>>>>>>> 4dafbe76142890b8168a7a62b37d5596444a3f4e

</body>
</html>
