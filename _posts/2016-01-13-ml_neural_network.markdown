---
layout: post
title:  "coursera_ML: neural network (1)"
date:   2016-01-12 20:34:25
categories: octave
tags: [octave, deep learning]

---

(주의 - 공부하면서 까먹지 않기 위한 개인적인 생각 및 정리이므로 수리적, 이론적으로 정확하지 않을 가능성이 있습니다.)

## 전자두뇌

뉴럴네트워크는 이름 자체에서 포스가 느껴진다. 각종 SF 영화에서 최종보스급으로 나오는 그런 AI를 만들어내는건가.. 인간의 사고와 행동을 관장하는 두뇌를 인공적으로 만들어낸다니 간지폭풍이다. 현재 기술수준은 완벽한 AI를 만들어내는데는 이르지 못했으나 꽤 많은 진전을 보이고 있다. 얼마전까지만해도 IT쪽 매체에 자주 오르내리던 딥러닝이라는 기술을 사용하면 상당히 똑똑한 수준의 AI를 구현할 수 있다. 구글에서 몇 백만장의 고양이 사진을 머신에 집어넣었더니 ‘cat’이라는 값을 뱉었다던가 (정확하지 않을 수 있다), 각종 명화(그림)를 학습시켰더니 알아서 자기가 예술작품처럼 보이는 합성작을 만들어낸다든가 하는 상상의 영역에만 머물러 왔던 일들이 현실화되고 있다. 요즘 핫한 스마트카의 자율주행 역시 딥러닝이 활용된 분야다.

여튼 뉴럴넷을 배우면 나도 자율주행차를 만들 수 있는 것인가! 할 수는 있다. 뉴럴넷을 고도화시키면 딥러닝이 되기는 하니까. 그러나 Coursera 머신러닝강의에서 딥러닝을 다루는 것 같지는 않고, 뉴럴넷을 써서 손글씨 알아보는 알고리즘을 만든다.

## logistic regression과 neural network

바로 전 강의에서 binary classification을 가능하게 해주는 logistic regression을 다뤘다. 그리고 스리슬쩍 인공신경망으로 넘어오는데 이 연결고리가 재밌다. 인공신경망이라는 이름에서 유추할 수 있듯이 수리적으로 신경망을 구성하는데, 당연히 먼저 신경 하나를 만드는데서부터 시작한다. 그리고 신경을 여러개로 묶어서 망을 구축하는 거다. 여기서 만드는 하나의 신경이 logistic regression을 사용한다. 신경 하나에 값을 입력하면 그 신경이 값을 처리해서 1이나 0으로 아웃풋을 뱉는다. 뱉어낸 아웃풋은 다시 다른 신경에 들어가는 인풋이 된다. 이런식으로 여러 신경들이 묶인 신경망을 통과하면서 최종적인 결과물이 나오는 방식이 뉴럴네트워크다.

![neuron](/assets/materials/20160113/1.png)

위의 도식은 인공신경망을 구성하는 최소단위인 뉴런을 도식화해놓은 것이다. x라는 입력변수가 input wires로 연결되어 입력되고 output wire를 거쳐 출력된다. 그리고 뉴런은 결과적으로 0과 1 사이의 value를 뱉어내므로 logistic regression과 동일한 방식, Sigmoid Activation을 사용한다.

![neural network](/assets/materials/20160113/2.png)

이러한 뉴런이 여러 레이어로 중첩된 형태가 뉴럴네트워크다. 뉴럴네트워크의 기본형을 시각적으로 구성하면 위와 같다. 간단하게 3가지 레이어로 구성해보았는데, 첫번째 레이어는 인풋레이어, 마지막 레이어는 아웃풋 레이어라고 한다. 말마따라 값을 받아와서 뱉는다. 중간에 레이어 1단이 하나 더 들어가는데 이 녀석이 인공신경망이 가지는 특징이다. Hidden layer, 즉 숨겨진 레이어라고 불리는 녀석으로, 인공신경망의 허리를 담당한다고 보면 된다. 이 숨겨진 레이어를 통해서 연산이 고도화된다. 숨겨진 레이어가 많으면 많을수록 연산도 복잡해지지만 결과도 좋아지는데, 기초적인 뉴럴넷이 1~2개의 히든레이어로 구성된다면 딥러닝은 더 많은 히든레이어를 돌린다.  (자세한 내용은 <a href='https://www.quora.com/How-does-deep-learning-work-and-how-is-it-different-from-normal-neural-networks-and-or-SVM'>여기</a>를 참조하면 좋다)

레이어안의 신경간 연결관계도 흥미로운 부분인데, 마치 complete network와 비슷하게 전후 레이어의 유닛간에 빽빽하게 연결이 되어있다. 즉 신경망에 들어온 모든 인풋은 그 다음 히든 레이어의 모든 유닛에 대해 영향을 미친다. 당연히 그 영향력은 인풋 유닛마다 가중치가 다르게 들어간다. linear regression이나 logistic regression에서 θ라는 녀석으로 그 가중치를 표현했다면 여기서는 가중치 자체가 매트릭스로 들어가므로 (인풋레이어 개수 * 레코드 개수) Θ로 표현해준다. 이걸 Vectorize해서 매트릭스 연산을 하는건데.. 그동안에는 vector column의 형태를 띄었다면 이제는 Matrix이므로 머리가 복잡해진다..

자 그럼 실제 수식이 어떻게 구성되어있는지 알아보자.

## Neural Network 수식

![neural network](/assets/materials/20160113/3.png)

a는 레이어 안에 있는 유닛을 의미하는데 i와 j를 달고 있다. 여기서 i는 j번째 레이어 L의 i번째 유닛을 의미한다. 수식에 보다보면 0번째 유닛도 등장하는데, 학습을 위한 bias 유닛으로 임의로 집어넣는다. 예를 들어 20픽셀 * 20픽셀로 이루어진 이미지를 처리한다고 하면 400개의 변수가 있지만 bias 유닛이 들어가므로 실제 피쳐의 개수는 401개가 된다. bias 유닛은 아웃풋 레이어를 제외한 모든 레이어에 들어간다.

그럼 각각의 유닛은 어떻게 계산되는걸까. a(2)1는 sigmoid함수인 g를 통해 나온 결과인데 이때 g에 넘기는 인자는 저렇게 복잡한 식으로 되어있다. 하나하나 풀어보면 크게 복잡하지는 않다. 앞서 올렸던 그림처럼 한 레이어의 모든 유닛은 그 다음 레이어의 (bias 유닛을 제외한) 모든 레이어에 영향을 주는데 여기서 Θ라는 가중치를 사용한다. 그리고 앞서 설명했던 것처럼 Θ는 매트릭스로 구성되어 있는데, 하나의 행이 하나의 웨이트 셋으로 보면 된다. Θ에 달린 superscript는 적용되는 레이어를 의미한다. 1이라면 레이어 1과 레이어 2사이에, 2라면 레이어 2와 3사이에 위치해서 웨이트를 준다. subscript는 행과 열로 표현되는 웨이트의 위치다.  다시 a(2)1의 사례를 보면.. 레이어1의 유닛에 적용되는 Θ의 1행 0번째 값을 x0에다 곱해주고 1행 1번째 값을 x1에다.. 곱해서 다 더하는 식으로 구성이 된다. vectorize하기 쉬운 형태로 구성되어 있는 걸 확인할 수 있다. 굳이 1번째 값이 아닌 0번째 값부터 시작하는 이유는 0번째 값은 bias unit이기 때문인 것으로 생각된다. 예를 들어 1행 1번째라고 하면 왠지 x0보다는 x1에다 곱해줘야할 것 같으니 말이다.

그러면 여기서 한가지 의문이 드는데 대체 theta값은 어떻게 정하냐다. 앞서 linear regression이나 logistic regression에서는 gradient descent나 normal equation을 통해서 최적의 theta 값, 즉 theta의 코스트 펑션이 가장 최소화되는 지점을 찾았다. 그런데 뉴럴넷에서는 그런게 없다. 뉴런 하나하나가 앞의 값을 읽고 그 다음으로 결과값을 뱉는 식이다. 이를 feedforward propagation이라고 한다. 그렇게 뒤로 넘겨서 최종적으로 나온 값이 y와 같다면 좋겠지만 같지 않을 경우는 어떻게 되는가? 이를 다시 뒤로 돌려서 theta를 보정하는 feedback으로 삼으면 된다. 이를 back propagation이라고 한다. 즉 뉴럴넷은 이 forward propagation과 back propagation을 통해서 theta값을 학습시킨다. 여기서는 forward propagation만 다루고 다음 포스팅에서 back propagation을 다루기로 한다.

## 뉴럴넷을 통한 boolean 연산
Andrew가 수식에 이어서 설명하는 부분이 뉴럴넷을 통한 boolean 연산이다. 여기서 or, and, not 등의 연산이 나오는데 이걸 모두 다 뉴럴넷을 통해 구현할 수 있다. 예를 들어 1개의 뉴런(sigmoid function)을 통해 출력되는 값이 1이나 0이라고 하자. 그러면 이를 조합할 수 있다.

![boolean expression](/assets/materials/20160113/4.png)

예컨대 위와 같이 1개의 bias 유닛과 2개의 x변수가 주어지고 임의의 theta값이 주어졌다고 해보자. 아웃풋 노드를 계산하면 어떤 결과가 나오는데 이는 sigmoid function을 통과한 값이다. 우상단의 sigmoid function 그래프를 보면 4를 넘어가면 거의 1에 가깝고 -4를 넘어가면 0에 근접한다. 이를 활용해서 x1와 x2가 0이거나 1인 4가지 경우를 생각해서 앞서 설명한 뉴럴넷 수식으로 a(2)1를 계산해보면 그림과 같이 0, 0, 0, 1이라는 값이 출력된다.

이 결과를 1일때 참이고 0일때 거짓인 조합으로 생각해보면 어떨까. false and false는 false고, false or true는 true다. 위에서 모든 값이 false일때는 false가 나오고 둘 중 하나가 true일 때도 false가 나왔다. 그리고 둘다 true일때는 true가 나왔다. 어떤 연산일까? AND 연산이다. 이런식으로 boolean 연산을 할 수가 있다. 익히 알고 있는 AND와 OR 말고도 XOR, XNOR 등등이 있다. 이 포스팅을 빌어 몇개를 확인해보자.

* XOR gate: 두개 값이 모두 서로 다른 경우에만 true. 즉 1/0이나 0/1일때만 1.
* NAND gate: negative and gate. 0/0/0/1을 뒤집은 케이스로 1/1/1/0이다.
* NOR gate: negative or gate. 0/1/1/1을 뒤집은 케이스로 0/1/1/1이다.
* XNOR gate: XOR의 반대값. 둘다 같을때만 1이다. 1/0/0/1.

다음은 뉴럴넷을 학습시키는 back propagation이다.